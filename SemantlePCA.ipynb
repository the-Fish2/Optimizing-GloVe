{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Exists\n",
      "KeyedVectors<vector_size=300, 3000000 keys>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if not {\"word2vec.model\", \"words.txt\"}.issubset(set(os.listdir())):\n",
    "\n",
    "    print(\"Beginning download\")\n",
    "\n",
    "    import gensim.downloader\n",
    "\n",
    "    print(\"imported\")\n",
    "\n",
    "    wv = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "\n",
    "    print(\"loaded\")\n",
    "\n",
    "    wv.save(\"./word2vec.model\")\n",
    "\n",
    "    print(\"saved\")\n",
    "    \n",
    "    f = open(\"words.txt\", \"x\")\n",
    "    for index, word in enumerate(wv.index_to_key):\n",
    "        if index == 300:\n",
    "            break\n",
    "        f.write(word)\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "print(\"File Exists\")\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load(\"word2vec.model\", mmap=\"r\")\n",
    "print(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = []\n",
    "for i in range(300):\n",
    "    x.append(wv[i])\n",
    "    # if (i == 100000):\n",
    "    #     print(\"hi\")\n",
    "    # if (i == 1000000):\n",
    "    #     print(\"hi\")\n",
    "x = np.asarray(x)\n",
    "\n",
    "# x = np.array([])\n",
    "# for i in range(300):\n",
    "#     x = np.append(x, wv[i])\n",
    "#     print(x[i][0], end = \" \")\n",
    "# print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = []\n",
    "with open(\"words.txt\", 'r') as f:\n",
    "    for i in range(300):\n",
    "        s = f.readline()\n",
    "        s = s[:len(s)-1]\n",
    "        z.append(s)\n",
    "#print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.KeyedVectors'>\n",
      "KeyedVectors<vector_size=300, 3000000 keys>\n",
      "KeyedVectors<vector_size=300, 300 keys>\n"
     ]
    }
   ],
   "source": [
    "print(type(wv))\n",
    "print(wv)\n",
    "iterator_for_words = (key for key in z)\n",
    "wv = wv.vectors_for_all(iterator_for_words)\n",
    "print(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['</s>', 'our', 'on', 'no', 'company', 'By', 'we', 'us', 'your', 'my'], ['in', 'where', 'the', 'In', 'during', 'at', 'when', 'here', 'then', 'what'], ['for', 'in', 'For', 'as', 'but', 'the', 'where', 'In', 'during', 'at'], ['that', 'it', 'not', 'if', 'but', 'what', 'just', 'It', 'really', 'do'], ['is', 'was', 'are', \"'re\", \"'m\", 'now', 'had', 'were', 'been', 'came'], ['##', '5', '3', '2', '###', 'six', '1', '#,###', '##,###', '#.#'], ['The', 'This', 'That', 'A', 'It', 'which', 'But', 'And', 'If', 'that'], ['with', 'between', 'in', 'had', 'while', 'by', 'over', 'both', 'through', 'where'], ['said', 'says', 'told', 'added', 'according', 'think', 'does', 'know', 'also', 'had'], ['be', 'being', 'are', 'have', 'should', 'been', 'was', 'were', \"'re\", 'these'], ['from', 'in', 'after', 'where', 'through', 'the', 'In', 'during', 'at', 'before'], ['I', \"'m\", 'my', 'me', 'we', 'you', \"'re\", 'am', 'really', 'your'], ['he', 'He', 'him', 'his', 'she', 'I', 'She', 'They', 'But', 'but'], ['will', 'can', 'would', 'should', 'could', 'may', 'want', 'did', 'do', 'need'], ['has', 'had', 'been', 'have', \"'ve\", 'since', 'was', 'were', 'they', 'we'], ['####', 'since', '##', '1', 'year', 'last', 'been', 'has', 'after', '5'], ['an', 'another', 'this', 'the', 'A', 'was', 'one', 'next', 'every', 'that'], ['or', 'any', 'your', 'can', 'you', 'if', 'no', 'not', 'never', 'my'], ['their', 'they', 'them', 'our', 'its', 'your', 'we', 'They', 'do', 'us'], ['who', 'He', 'also', 'he', 'former', 'him', 'She', 'They', 'But', 'but'], ['$', 'million', '###,###', '#.##', '#.#', '##,###', 'billion', '#,###', '###', '##.#'], ['more', 'than', 'most', 'little', 'better', 'about', 'much', 'even', 'one', 'very'], ['up', 'down', 'out', 'off', 'around', 'back', 'into', 'on', 'over', 'where'], ['all', 'these', 'those', 'other', 'some', 'both', 'many', 'are', 'including', 'such'], ['two', 'three', 'four', 'five', 'six', 'few', '##', 'some', 'many', '5'], ['first', 'second', 'third', 'last', '##th', 'next', 'half', 'three', 'six', 'ago'], ['time', 'day', 'days', 'when', 'months', 'year', 'week', 'month', 'night', 'years'], ['We', 'They', 'we', 'You', 'If', 'I', 'But', 'they', 'And', 'our'], ['new', 'next', 'will', 'another', 'the', 'own', 'last', 'first', 'this', 'can'], ['her', 'she', 'his', 'She', 'my', 'him', 'he', 'I', 'He', 'their'], ['people', 'children', 'those', 'them', 'us', 'just', 'family', 'school', 'these', 'other'], ['there', 'There', 'no', 'here', 'going', 'we', 'But', 'And', 'It', 'If'], ['U.S.', 'American', 'world', 'country', 'billion', 'government', 'AP', 'our', 'most', 'one'], ['so', 'too', 'but', 'because', 'really', 'very', 'not', 'But', 'It', 'if'], ['like', 'really', 'think', 'just', 'do', 'want', 'so', 'I', 'very', 'know'], ['only', 'one', 'just', 'not', 'but', 'even', 'two', 'three', 'five', 'four'], ['percent', '%', '#.#', '##.#', 'million', 'billion', '#.##', '###,###', '###', '##'], ['get', 'got', 'go', 'come', 'do', 'just', 'came', 'went', 'had', \"'ve\"], ['game', 'games', 'play', 'season', 'players', 'team', 'points', 'win', 'go', 'year'], ['against', 'game', 'win', 'in', '#-#', 'case', 'games', 'play', 'season', 'players'], ['made', 'make', 'came', 'had', 'no', 'did', 'get', 'do', 'if', 'come'], ['state', 'State', 'government', 'country', 'city', 'local', 'U.S.', 'Saturday', 'former', 'officials'], ['well', 'as', 'good', 'much', 'such', 'better', 'As', 'so', 'but', 'great'], ['home', 'family', 'back', 'game', 'off', 'when', 'children', 'life', 'own', 'people'], ['way', 'how', 'going', 'it', 'really', 'so', 'what', 'if', 'want', 'do'], ['work', 'do', 'go', 'get', 'come', 'so', 'want', 'know', 'not', 'did'], ['take', 'took', 'go', 'put', 'get', 'come', 'went', 'came', 'got', 'had'], ['high', 'top', 'down', 'school', 'well', 'up', 'best', 'second', 'third', 'one'], ['still', 'now', 'but', 'even', 'just', 'only', 'right', 'is', \"'re\", 'because'], ['old', 'man', 'ago', 'year', 'last', 'who', 'him', 'he', 'police', 'his'], ['see', 'know', 'think', 'do', 'get', 'say', 'really', 'want', 'not', 'did'], ['business', 'company', 'companies', 'market', 'sales', 'services', 'its', 'government', 'quarter', 'percent'], ['under', 'put', 'on', 'into', 'without', 'the', 'come', 'go', 'take', 'get'], ['help', 'need', 'support', 'better', 'can', 'do', 'want', 'should', 'services', 'money'], ['end', 'start', 'until', 'point', 'next', 'back', 'go', 'early', 'run', 'before'], ['long', 'many', 'well', 'much', 'just', 'few', 'some', 'those', 'these', 'all'], ['information', 'services', 'report', 'money', 'For', 'any', 'service', 'companies', 'business', 'support'], ['part', 'this', 'because', 'the', 'also', 'really', 'another', 'that', 'last', 'it'], ['based', 'company', 'its', 'in', 'from', 'according', 'companies', 'business', 'market', 'sales'], ['left', 'right', 'went', 'took', 'came', 'back', 'now', 'just', 'going', 'do'], ['use', 'used', 'need', 'do', 'take', 'help', 'can', 'could', 'called', 'want'], ['today', 'Thursday', 'Wednesday', 'Monday', 'Tuesday', 'Friday', 'Saturday', 'Sunday'], ['same', 'every', 'each', 'this', 'the', 'only', 'another', 'one', 'all', 'other'], ['public', 'government', 'local', 'people', 'state', 'city', 'country', 'billion', 'companies', 'area'], ['set', 'put', 'come', 'start', 'the', 'for', 'go', 'take', 'get', 'came'], ['place', 'where', 'time', 'in', 'lead', 'the', 'when', 'here', 'then', 'what'], ['group', 'team', 'who', 'members', 'company', 'program', 'players', 'game', 'season', 'games'], ['found', 'were', 'according', 'say', 'see', 'was', 'are', 'had', 'been', 'have'], ['lot', 'some', 'little', 'really', 'much', 'great', 'many', 'few', 'those', 'these'], ['number', 'many', 'two', 'these', 'those', 'four', 'some', 'few', 'all', 'three'], ['##-##', '#-#', '##', '3', '##.#', '5', 'win', 'game', 'points', 'second'], ['show', 'see', 'say', 'know', 'so', 'program', 'think', 'do', 'get', 'not'], ['past', 'over', 'last', 'ago', 'years', 'have', 'around', 'between', 'off', 'first'], ['expected', 'will', 'could', 'next', 'would', 'going', 'can', 'should', 'may', 'last'], ['hit', 'off', 'left', 'run', 'went', 'came', 'down', 'out', 'back', 'up'], ['system', 'program', 'service', 'services', 'state', 'way', 'group', 'show', 'business', 'work'], ['pm', 'am', 'Saturday', 'Sunday', 'Friday', 'Thursday', \"'m\", 'I', \"'re\", 'is'], ['big', 'great', 'good', 'lot', 'really', 'little', 'best', 'better', 'some', 'much'], ['per', '#.##', '##.#', '#.#', '$', '###', '5', 'percent', '##', '%'], ['&', 'By', '#.##', '3', 'A', '2', 'by', 'AP', 'In', 'For']]\n"
     ]
    }
   ],
   "source": [
    "def clusters300():\n",
    "    #swap to snake_case\n",
    "    touched_points = []\n",
    "    clusters = []\n",
    "\n",
    "    for i in range (300):\n",
    "        if z[i] in touched_points:\n",
    "            continue\n",
    "        curr_cluster = make_cluster(z[i])\n",
    "        clusters.append(curr_cluster)\n",
    "        # print(\"check2\")\n",
    "        # print(curr_cluster)\n",
    "        touched_points.extend(curr_cluster)\n",
    "        # print(touched_points)\n",
    "\n",
    "    #call other function to recursively add elements \n",
    "    #curr cluster = list\n",
    "    #append to total list of clusters\n",
    "    print(clusters)\n",
    "\n",
    "#z = wv\n",
    "\n",
    "def make_cluster(word):\n",
    "    #global wv\n",
    "\n",
    "    #print(word)\n",
    "\n",
    "    curr_cluster=[]\n",
    "    visited = []\n",
    "\n",
    "    queue = []\n",
    "    queue.append(word)\n",
    "    visited.append(word)\n",
    "\n",
    "    z2 = z.copy()\n",
    "\n",
    "    #print(curr_cluster)\n",
    "    \n",
    "    while len(curr_cluster) < 10 and queue:\n",
    "        curr_word = queue.pop(0)\n",
    "        if curr_word in curr_cluster:\n",
    "            break\n",
    "\n",
    "        curr_cluster.append(curr_word)\n",
    "\n",
    "        #print(curr_word, end = \" \")\n",
    "\n",
    "        z2.remove(curr_word)\n",
    "\n",
    "        for i in range(5):\n",
    "\n",
    "            closest_word = wv.most_similar_to_given(curr_word, z2)\n",
    "#companies repeated?? word added??\n",
    "            if (closest_word in z2):\n",
    "                z2.remove(closest_word)\n",
    "\n",
    "            if not (closest_word in visited):\n",
    "                #curr_cluster subset of visited\n",
    "                queue.append(closest_word)\n",
    "                visited.append(closest_word)\n",
    "\n",
    "        z2 = z.copy()\n",
    "\n",
    "    return curr_cluster\n",
    "\n",
    "clusters300()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need a way better way to do this T-T\n",
    "maxTransform = np.asarray([\n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [],  \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    ])\n",
    "maxNum = 0\n",
    "\n",
    "def dataAnalysis(dim, data):\n",
    "    global maxTransform, maxNum\n",
    "    if maxNum > dim:\n",
    "        maxTransform2 = np.transpose(maxTransform)\n",
    "        maxTransform2 = maxTransform2[:dim]\n",
    "        maxTransform2 = np.transpose(maxTransform2)\n",
    "        return maxTransform2\n",
    "\n",
    "    else:\n",
    "        newDim = dim - maxNum\n",
    "\n",
    "        arr2 = data[ : , newDim : data[0].__len__() ]\n",
    "        pca = PCA(n_components = newDim)\n",
    "        pca.fit(arr2)\n",
    "        arr2 = np.asarray(pca.transform(arr2))\n",
    "\n",
    "        np.transpose(maxTransform)\n",
    "        np.transpose(arr2)\n",
    "        maxTransform = np.append(maxTransform, arr2, 1)\n",
    "        np.transpose(maxTransform)\n",
    "\n",
    "        # print(type(maxTransform))\n",
    "        # print(type(arr2))\n",
    "        # print(\"Max Transform # of vectors \" + str(maxTransform.__len__()))\n",
    "        # print(\"Max Transform Size of vectors \" + str(maxTransform[0].__len__()))\n",
    "\n",
    "        maxNum = dim\n",
    "        return maxTransform\n",
    "\n",
    "#x = np.array([[1, 2, 3, 4, 5], [2, 1, 3, 3, 1], [2, 4, 1, 2, 3], [4, 3, 2, 1, 4]])\n",
    "# print(dataAnalysis(2, x))\n",
    "# dataAnalysis(3, x)\n",
    "# print(dataAnalysis(2, x))\n",
    "# print(maxTransform)\n",
    "\n",
    "#perhaps describe how to do it with gradient function and the like to pick a random dimension\n",
    "#instead of going down one dimension at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(dim, data):\n",
    "    #data = np.array(data)\n",
    "    if dim == 3 or dim == 2:\n",
    "        fig = plt.figure(figsize=(15, 15))\n",
    "        plt.clf()\n",
    "\n",
    "        if dim == 3:\n",
    "            ax = fig.add_subplot(projection = \"3d\") #this is rectilinear, 3d, etc. projection= \"3d\"\n",
    "        elif dim == 2:\n",
    "            ax = fig.add_subplot(projection = \"rectilinear\")\n",
    "        \n",
    "        ax.set_position([0, 0, 0.95, 1])\n",
    "        plt.cla()\n",
    "           \n",
    "        if dim == 3:\n",
    "            ax.scatter(data[:, 0], data[:, 1], data[:, 2])\n",
    "            #not allowed to do it in 3 dimensions; b/c they expect a point of size two in annotation function\n",
    "            # for i in range (300):\n",
    "            #     ax.annotate(f.readline(), (data[i, 0], data[i, 1], data[i, 2]))\n",
    "            #     f.close()\n",
    "        elif dim == 2:\n",
    "            ax.scatter(data[:, 0], data[:, 1])\n",
    "            # plt.ylim(-1, 3)\n",
    "            # plt.xlim(-1.5, 2)\n",
    "            for i in range (300):\n",
    "                ax.annotate(z[i], (data[i, 0], data[i, 1]))\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(\"Too many/too few dimensions to visualize\")\n",
    "        ans = \"y\"\n",
    "        #ans = input(\"Proceed with 2-D? y/n\"\n",
    "        if ans == \"y\":\n",
    "            fig = plt.figure(figsize=(15 * dim, 15 * dim))\n",
    "            fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "            for i in range(dim):\n",
    "                for j in range(i+1, dim):\n",
    "                    #dim, dim-1, (i+1)*(j+1\n",
    "                    ax = fig.add_subplot()\n",
    "                    ax.scatter(data[:, i], data[:, j])\n",
    "\n",
    "                    #with open(\"words.txt\", \"r\") as f:\n",
    "                        #ax.set_position([0, 0, 20.95, 20])\n",
    "                    for k in range (300):\n",
    "                        ax.annotate(z[i], (data[k, i], data[k, j]))\n",
    "                        #f.close()\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        #https://matplotlib.org/stable/api/projections_api.html#module-matplotlib.projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playable(dim, data):\n",
    "    y = dataAnalysis(dim, data)\n",
    "    print(\"size\" + str(y[0].__len__()))\n",
    "    print(\"length\" + str(y.__len__()))\n",
    "    #figure out AI later\n",
    "    #maybe instead of coding ai, all I have to do is check number of words with similarity value \n",
    "    #biggest similarity\n",
    "    #smallest similarity\n",
    "    #level of variance?\n",
    "    #number of words with distance d away from starting word\n",
    "    #want to preserve distance, too\n",
    "    if dim >= 2: \n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDim(data, start=1, end=300):\n",
    "    print(start)\n",
    "    print(end)\n",
    "    if start >= end:\n",
    "        return end\n",
    "    else:\n",
    "        mid = (start + end)//2\n",
    "        if not playable(mid, data):\n",
    "            return findDim(data, mid, end)\n",
    "        else:\n",
    "            return findDim(data, start, mid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f07e8f80a78d731554c2ce7ed8433d0fa1e00c779c09dd1ced00d3c38371f3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
