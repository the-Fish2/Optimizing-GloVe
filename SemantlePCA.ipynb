{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Exists\n",
      "KeyedVectors<vector_size=300, 3000000 keys>\n"
     ]
    }
   ],
   "source": [
    "# RUN general imports\n",
    "\n",
    "import os\n",
    "\n",
    "if not {\"word2vec.model\", \"words.txt\"}.issubset(set(os.listdir())):\n",
    "\n",
    "    print(\"Beginning download\")\n",
    "\n",
    "    import gensim.downloader\n",
    "\n",
    "    print(\"imported\")\n",
    "\n",
    "    wv = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "\n",
    "    print(\"loaded\")\n",
    "\n",
    "    wv.save(\"./word2vec.model\")\n",
    "\n",
    "    print(\"saved\")\n",
    "    \n",
    "    f = open(\"words.txt\", \"x\")\n",
    "    for index, word in enumerate(wv.index_to_key):\n",
    "        if index == 300:\n",
    "            break\n",
    "        f.write(word)\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "print(\"File Exists\")\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load(\"word2vec.model\", mmap=\"r\")\n",
    "print(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN generates x\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = []\n",
    "for i in range(300):\n",
    "    x.append(wv[i])\n",
    "    # if (i == 100000):\n",
    "    #     print(\"hi\")\n",
    "    # if (i == 1000000):\n",
    "    #     print(\"hi\")\n",
    "x = np.asarray(x)\n",
    "\n",
    "# x = np.array([])\n",
    "# for i in range(300):\n",
    "#     x = np.append(x, wv[i])\n",
    "#     print(x[i][0], end = \" \")\n",
    "# print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN imports\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN generates array z (words)\n",
    "z = []\n",
    "with open(\"words.txt\", 'r') as f:\n",
    "    for i in range(300):\n",
    "        s = f.readline()\n",
    "        s = s[:len(s)-1]\n",
    "        z.append(s)\n",
    "#print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.KeyedVectors'>\n",
      "KeyedVectors<vector_size=300, 3000000 keys>\n",
      "KeyedVectors<vector_size=300, 300 keys>\n"
     ]
    }
   ],
   "source": [
    "#convert keyed vectors to smaller length\n",
    "print(type(wv))\n",
    "print(wv)\n",
    "iterator_for_words = (key for key in z)\n",
    "wv = wv.vectors_for_all(iterator_for_words)\n",
    "print(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s> \n",
      "in where the In during at this another \n",
      "for in For as but the where \n",
      "that it not if but what do did \n",
      "is was are 're 'm now were \n",
      "on off the through in down \n",
      "## 5 3 2 ### six \n",
      "The This That A It which \n",
      "with between in had while by where the \n",
      "said says told added according think know really \n",
      "be being are have should been were 're \n",
      "from in after where through the \n",
      "I 'm my me we you us him \n",
      "he He him his she I She me \n",
      "will can would should could may \n",
      "has had been have 've since \n",
      "#### since ## 1 year last 5 3 \n",
      "an another this the A was \n",
      "or any your can you if no \n",
      "their they them our its your we \n",
      "who He also he former him She \n",
      "$ million ###,### #.## #.# ##,### billion \n",
      "more than most little better about one much \n",
      "up down out off around back \n",
      "all these those other some both \n",
      "two three four five six few \n",
      "first second third last ##th next \n",
      "time day days when months year week \n",
      "We They we You If I you And \n",
      "over past around between last off with ago \n",
      "new next will another the own last \n",
      "her she his She my him He I \n",
      "people children those them us just these other \n",
      "into through out back then off down \n",
      "there There no here going we any \n",
      "years months days year ago month \n",
      "U.S. American world country billion \n",
      "But And If That It but \n",
      "so too but because really very \n",
      "like really think just do want \n",
      "only one just not but even really so \n",
      "percent % #.# ##.# million billion \n",
      "get got go come do just \n",
      "game games play season players team \n",
      "before after when until then last \n",
      "company companies business market sales its \n",
      "against game win in #-# case games play \n",
      "made make came had no did went come \n",
      "state State government country city local world area \n",
      "well as good much such better great \n",
      "home family back game off out down \n",
      "many some few those these all two \n",
      "way how going it really so what \n",
      "work do go get come so want know \n",
      "take took go put get come \n",
      "including other three four five for those these \n",
      "high top best second third one first good \n",
      "#,### ##,### ###,### ### #.# ## ##.# #.## \n",
      "By by AP being be was Wednesday Thursday \n",
      "still now but even just only because so \n",
      "old man ago year last who him \n",
      "see know think do get say \n",
      "under put on into come go take get \n",
      "right left now just going do still \n",
      "help need support better can do want \n",
      "end start until point next back before when \n",
      "long many well much just few some \n",
      "information services service companies business support system \n",
      "points point ##-## #-# win games \n",
      "does did do not would can \n",
      "part this because the also really \n",
      "police officials man city him old he area \n",
      "based company companies business market sales its \n",
      "school children state people family State government country \n",
      "quarter half ##.# #.# #.## third \n",
      "use used need do take help want can \n",
      "today Thursday Wednesday Monday Tuesday Friday \n",
      "same every each this the only \n",
      "public government local people state \n",
      "run go start play take lead come going \n",
      "set put come \n",
      "place where time in lead the \n",
      "night Saturday Sunday Friday Monday Thursday \n",
      "group team who players game season He also \n",
      "found were according say are was had been \n",
      "As But And If as The \n",
      "lot some little really much great many few \n",
      "money $ ###,### million billion help \n",
      "program system \n",
      "report according says told said \n",
      "number many two these those four some few \n",
      "am 'm pm I 're is \n",
      "life family home children back game off people \n",
      "show see know think do get say \n",
      "expected will could next would going can \n",
      "called came used said went come got took \n",
      "hit off left run went came down out \n",
      "big great good lot really little \n",
      "per #.## ##.# #.# $ ### \n",
      "members officials people police children those them us \n",
      "early before until after when during \n",
      "& \n",
      "never not 've if have even do did \n",
      "without before no any not while after when \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0],\n",
       " [1, 117, 11, 70, 135, 12, 28, 197],\n",
       " [2, 1, 192, 14, 33, 11, 117],\n",
       " [3, 15, 13, 88, 33, 83, 58, 92],\n",
       " [4, 10, 19, 129, 236, 97, 37],\n",
       " [5, 104, 11, 108, 1, 119],\n",
       " [6, 293, 234, 200, 40, 222],\n",
       " [7, 105, 176, 73, 51, 48],\n",
       " [8, 171, 1, 35, 118, 18, 117, 11],\n",
       " [9, 115, 162, 264, 291, 153, 177, 230],\n",
       " [16, 120, 19, 21, 130, 42, 37, 129],\n",
       " [17, 1, 55, 117, 108, 11],\n",
       " [20, 236, 126, 170, 38, 43, 164, 93],\n",
       " [22, 57, 93, 26, 74, 20, 218, 170],\n",
       " [23, 50, 47, 130, 75, 137],\n",
       " [24, 35, 42, 21, 190, 140],\n",
       " [25, 140, 6, 165, 36, 64, 293, 234],\n",
       " [27, 197, 28, 11, 73, 10],\n",
       " [29, 101, 122, 50, 43, 88, 86],\n",
       " [30, 32, 82, 71, 44, 122, 38],\n",
       " [31, 57, 53, 22, 249, 93, 218],\n",
       " [34, 81, 225, 156, 98, 174, 214],\n",
       " [39, 60, 106, 286, 254, 46, 45, 150],\n",
       " [41, 119, 49, 104, 187, 95],\n",
       " [52, 166, 133, 66, 78, 184],\n",
       " [54, 80, 134, 161, 222, 278],\n",
       " [56, 110, 194, 64, 299, 147],\n",
       " [59, 113, 255, 61, 245, 36, 116],\n",
       " [62, 128, 38, 228, 167, 20, 43, 169],\n",
       " [63, 262, 187, 171, 64, 104, 8, 289],\n",
       " [65, 147, 23, 197, 11, 250, 64],\n",
       " [67, 74, 26, 218, 126, 93, 57, 20],\n",
       " [68, 270, 133, 82, 164, 76, 166, 66],\n",
       " [69, 108, 49, 95, 145, 104, 119],\n",
       " [72, 185, 86, 227, 125, 38, 101],\n",
       " [77, 245, 255, 36, 289, 212],\n",
       " [79, 259, 157, 198, 214],\n",
       " [84, 169, 167, 176, 51, 33],\n",
       " [85, 253, 33, 96, 230, 138],\n",
       " [87, 230, 153, 76, 58, 189],\n",
       " [89, 45, 76, 13, 33, 155, 230, 85],\n",
       " [90, 146, 98, 206, 81, 214],\n",
       " [91, 193, 152, 216, 58, 76],\n",
       " [94, 239, 172, 114, 298, 102],\n",
       " [99, 55, 61, 296, 145, 64],\n",
       " [100, 285, 160, 175, 295, 44],\n",
       " [103, 94, 202, 1, 123, 288, 239, 172],\n",
       " [107, 109, 256, 35, 86, 92, 297, 216],\n",
       " [111, 268, 121, 198, 219, 290, 157, 272],\n",
       " [112, 14, 127, 150, 136, 254, 267],\n",
       " [124, 244, 95, 94, 104, 49, 119],\n",
       " [131, 78, 278, 133, 166, 52, 54],\n",
       " [132, 139, 125, 15, 230, 85, 83],\n",
       " [141, 58, 152, 91, 216, 85, 189, 177],\n",
       " [142, 263, 152, 238, 91, 216],\n",
       " [143, 66, 80, 134, 161, 2, 133, 166],\n",
       " [144, 213, 201, 110, 194, 45, 56, 127],\n",
       " [148, 174, 225, 40, 98, 6, 206, 156],\n",
       " [149, 18, 179, 120, 16, 10, 226, 224],\n",
       " [151, 97, 33, 155, 76, 89, 96, 85],\n",
       " [154, 251, 289, 36, 64, 31, 93],\n",
       " [158, 177, 153, 58, 91, 159],\n",
       " [163, 238, 5, 69, 216, 152, 142, 91],\n",
       " [168, 196, 97, 76, 125, 58, 151],\n",
       " [173, 199, 240, 254, 50, 58, 189],\n",
       " [178, 284, 296, 283, 147, 95, 99, 61],\n",
       " [180, 131, 112, 150, 76, 278, 78],\n",
       " [181, 269, 277, 285, 160, 240, 273],\n",
       " [182, 283, 260, 123, 202, 239],\n",
       " [183, 92, 58, 13, 47, 50],\n",
       " [186, 28, 96, 11, 53, 230],\n",
       " [188, 247, 251, 219, 93, 154, 22, 272],\n",
       " [191, 100, 285, 160, 175, 295, 44],\n",
       " [195, 270, 111, 68, 244, 268, 121, 198],\n",
       " [203, 242, 206, 98, 156, 194],\n",
       " [204, 233, 199, 58, 142, 173, 189, 50],\n",
       " [205, 224, 226, 217, 215, 210],\n",
       " [207, 274, 223, 28, 11, 89],\n",
       " [208, 121, 290, 68, 111],\n",
       " [209, 152, 284, 172, 142, 257, 216, 125],\n",
       " [211, 238, 216],\n",
       " [220, 117, 59, 1, 257, 11],\n",
       " [221, 252, 281, 210, 217, 224],\n",
       " [229, 102, 31, 298, 94, 114, 57, 53],\n",
       " [231, 37, 291, 159, 19, 10, 35, 42],\n",
       " [232, 84, 169, 167, 14, 7],\n",
       " [235, 78, 286, 230, 150, 267, 131, 278],\n",
       " [237, 34, 225, 81, 214, 173],\n",
       " [241, 273],\n",
       " [243, 291, 115, 162, 9],\n",
       " [246, 131, 54, 166, 133, 134, 78, 278],\n",
       " [248, 236, 275, 20, 129, 4],\n",
       " [258, 244, 124, 270, 95, 94, 104, 68],\n",
       " [261, 158, 177, 153, 58, 91, 159],\n",
       " [265, 23, 75, 147, 47, 125, 50],\n",
       " [266, 256, 233, 9, 297, 216, 193, 263],\n",
       " [271, 104, 196, 209, 297, 256, 119, 49],\n",
       " [276, 267, 127, 235, 230, 286],\n",
       " [279, 156, 206, 98, 34, 40],\n",
       " [280, 247, 68, 188, 270, 133, 82, 164],\n",
       " [282, 99, 296, 55, 61, 135],\n",
       " [287],\n",
       " [292, 13, 190, 88, 21, 155, 58, 92],\n",
       " [294, 99, 86, 101, 13, 118, 55, 61]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN Bash alg\n",
    "#bash list of clusters\n",
    "\n",
    "def cluster_gen(data):\n",
    "    #swap to snake_case\n",
    "    touched_points = []\n",
    "    clusters = []\n",
    "\n",
    "    for i in range (300):\n",
    "        if i in touched_points:\n",
    "            continue\n",
    "\n",
    "        curr_cluster = make_cluster(i, data)\n",
    "        clusters.append(curr_cluster)\n",
    "        # print(\"check2\")\n",
    "        # print(curr_cluster)\n",
    "        touched_points.extend(curr_cluster)\n",
    "        # print(touched_points)\n",
    "\n",
    "    # call other function to recursively add elements \n",
    "    # curr cluster = list\n",
    "    # append to total list of clusters\n",
    "\n",
    "    return clusters\n",
    "\n",
    "#maybe change algorithm to be like. find clusters of 'two' for closest words\n",
    "#then combine them to make clusters of 4\n",
    "#then 8\n",
    "#so forth\n",
    "#elements can be in multiple clusters\n",
    "#this is agglomerative tho\n",
    "def find_most_similar(word, total_words, curr_cluster):\n",
    "\n",
    "    from scipy.spatial.distance import cosine\n",
    "\n",
    "    m = 0\n",
    "    while m in curr_cluster:\n",
    "        m = m + 1\n",
    "\n",
    "    min_val = cosine(u = total_words[word], v = total_words[m])\n",
    "    index = m\n",
    "\n",
    "    for i in range(m+1, 300):\n",
    "        skip = False\n",
    "\n",
    "        if i in curr_cluster:\n",
    "            skip = True\n",
    "\n",
    "        # if word[0] == total_words[i][0]:\n",
    "        #     for j in range(2, 300):\n",
    "        #         if (word[j] == total_words[i][j]):\n",
    "        #             skip = True\n",
    "        \n",
    "        curr_val = cosine(u = total_words[word], v = total_words[i])\n",
    "        #print(i, end = \" \")\n",
    "        #print(curr_val)\n",
    "\n",
    "        if skip:\n",
    "            curr_val = min_val\n",
    "        \n",
    "        if curr_val < min_val:\n",
    "            min_val = curr_val\n",
    "            index = i\n",
    "\n",
    "    #print(str(min_val) + \" \" + str(word) + \" \" + str(index))\n",
    "    # def issue - same computation 5 times??\n",
    "\n",
    "    if min_val > 0.7:\n",
    "        return word\n",
    "\n",
    "    return index\n",
    "\n",
    "# ans = find_most_similar(217, x, [217])\n",
    "# print(z[ans])\n",
    "\n",
    "#z = wv\n",
    "def make_cluster(word, data):\n",
    "\n",
    "    # print()\n",
    "\n",
    "    curr_cluster=[]\n",
    "    visited = []\n",
    "    queue = []\n",
    "    queue.append(word)\n",
    "\n",
    "    while len(curr_cluster) < 8 and queue:\n",
    "        curr_word = queue.pop(0)\n",
    "\n",
    "        # print(z[curr_word], end = \" \")\n",
    "\n",
    "        if curr_word in curr_cluster:\n",
    "            break\n",
    "\n",
    "        curr_cluster.append(curr_word)\n",
    "        visited.append(curr_word)\n",
    "        \n",
    "        #alg:\n",
    "        for i in range(5):\n",
    "\n",
    "            closest_word = find_most_similar(curr_word, data, visited)\n",
    "\n",
    "            if not (closest_word in curr_cluster):\n",
    "                queue.append(closest_word)\n",
    "            \n",
    "            if closest_word == word:\n",
    "                break\n",
    "            \n",
    "            visited.append(closest_word)\n",
    "\n",
    "        #alg2:\n",
    "        visited = []\n",
    "\n",
    "    # if len(curr_cluster) == 1:\n",
    "    #     return []\n",
    "    \n",
    "    return curr_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  6  6 52  9  6 55 30  6  8  9  6 29 51  1 52  9  6  6  9 24 18 23  4\n",
      " 18 61 23  6 45 37 27  6 27 52 20 18 15  9 27  7 55 43 18 17  6 52  0  4\n",
      "  6 43  4 30 39  6 39 61 64 68 51 29  7 52 63  0 45  6 39 23 12  0 40 27\n",
      " 52 30 23  4 52 25 39 28 39 46 27 52 38 52 37 52 52 52 44 11 51 23 10 43\n",
      " 52 52  3 61 49 37  5  5 43 30  1 53  0 53 64 14  1 29 15  8 15 52  6 43\n",
      "  9 14 17 36 57 52 24 22 63 34  4 39 52 39 39 61  1  4  1 52 61 11 48  6\n",
      " 58 52 44 45 59 47  7 52 11 42 16 52  3 28 42 42 49 39  8  0 24 55 39 38\n",
      " 13 38 24  0 10 65 59 49 30 42 29 67  1 41 32 51 39 56  6  0 69 51 18 26\n",
      " 40 11 64 12 13 45 28 51 55 22 62 35 33 29  3 45 14 11 50 29 15 58 46 50\n",
      " 11 50 68 14 29 50 39 45 50 59 50 52 17  5 42  2 38 33 55  7 34 20 11 10\n",
      " 65 21 35  2 57 25 39 19 31  5  6 16 50  1 22 25 11 62 57 28 36  2  0 48\n",
      "  8  4  6 22 60 54 12 13 14 21 45 31 22 54 39  3 19 50 61 32 29 49  7 26\n",
      "  5 25 14  8 18 55 37 66 61 11 10 64]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['about', 'over', 'into', 'through', 'under', 'between', 'around', 'past'],\n",
       " ['as', 'most', 'well', 'such', 'very', 'long', 'too'],\n",
       " ['found', 'report', 'show'],\n",
       " ['#.#', '#.##', '##.#', 'per'],\n",
       " ['will', 'would', 'can', 'could', 'should', 'may', 'expected'],\n",
       " ['team', 'against', 'group', 'former', 'case'],\n",
       " ['in',\n",
       "  'for',\n",
       "  'on',\n",
       "  'with',\n",
       "  'the',\n",
       "  'from',\n",
       "  'by',\n",
       "  'an',\n",
       "  'who',\n",
       "  'its',\n",
       "  'which',\n",
       "  'also',\n",
       "  'new',\n",
       "  'while',\n",
       "  'including',\n",
       "  'part',\n",
       "  'own',\n",
       "  'called'],\n",
       " ['more', 'than', 'much', 'lot', 'little'],\n",
       " ['said', 'says', 'told', 'added', 'according'],\n",
       " ['is', 'was', 'be', 'are', 'were', 'being'],\n",
       " ['game', 'play', 'games', 'players'],\n",
       " ['get', 'work', 'go', 'got', 'run', 'come', 'put', 'came', 'went'],\n",
       " ['people', 'school', 'children'],\n",
       " ['right', 'left', 'hit'],\n",
       " ['state', 'government', 'public', 'city', 'area', 'local'],\n",
       " ['year', 'season', 'week', 'month'],\n",
       " ['old', 'man'],\n",
       " ['you', 'your', 'You'],\n",
       " ['have', 'has', 'had', 'been', \"'ve\", 'never'],\n",
       " ['officials', 'members'],\n",
       " ['$', 'money'],\n",
       " ['program', 'system'],\n",
       " ['good', 'best', 'better', 'great', 'big'],\n",
       " ['he', 'his', 'her', 'she', 'him'],\n",
       " ['I', 'my', 'us', 'me'],\n",
       " ['years', 'months', 'days', 'ago'],\n",
       " ['based', '&'],\n",
       " ['their', 'they', 'we', 'our', 'them'],\n",
       " ['U.S.', 'world', 'country', 'American'],\n",
       " ['at', 'time', 'day', 'end', 'today', 'set', 'place', 'start'],\n",
       " ['The', 'It', 'A', 'This', 'That'],\n",
       " ['am', 'pm'],\n",
       " ['points', 'point'],\n",
       " ['use', 'used'],\n",
       " [\"'re\", \"'m\"],\n",
       " ['quarter', 'half'],\n",
       " ['#-#', '##-##'],\n",
       " ['or', 'no', 'any', 'without'],\n",
       " ['But', 'If', 'And', 'As'],\n",
       " ['all',\n",
       "  'two',\n",
       "  'other',\n",
       "  'some',\n",
       "  'three',\n",
       "  'many',\n",
       "  'those',\n",
       "  'four',\n",
       "  'five',\n",
       "  'these',\n",
       "  'both',\n",
       "  'six',\n",
       "  'number',\n",
       "  'few'],\n",
       " ['In', 'For'],\n",
       " ['information'],\n",
       " ['think', 'see', 'say', 'know', 'really'],\n",
       " ['up', 'out', 'back', 'off', 'down'],\n",
       " ['percent', '%'],\n",
       " ['this', 'last', 'next', 'another', 'same', 'each', 'every'],\n",
       " ['million', 'billion'],\n",
       " ['By'],\n",
       " ['take', 'took'],\n",
       " ['company', 'business', 'market', 'companies'],\n",
       " ['Friday',\n",
       "  'Tuesday',\n",
       "  'Monday',\n",
       "  'night',\n",
       "  'Thursday',\n",
       "  'Wednesday',\n",
       "  'Saturday',\n",
       "  'Sunday'],\n",
       " ['not', 'do', 'did', 'does', 'want', 'need'],\n",
       " ['that',\n",
       "  'it',\n",
       "  'but',\n",
       "  'one',\n",
       "  'when',\n",
       "  'there',\n",
       "  'just',\n",
       "  'what',\n",
       "  'so',\n",
       "  'like',\n",
       "  'if',\n",
       "  'only',\n",
       "  'because',\n",
       "  'now',\n",
       "  'where',\n",
       "  'going',\n",
       "  'way',\n",
       "  'how',\n",
       "  'then',\n",
       "  'still',\n",
       "  'even',\n",
       "  'here'],\n",
       " ['made', 'make'],\n",
       " ['services', 'service'],\n",
       " ['##', '###', '1', '2', '3', '5'],\n",
       " ['There'],\n",
       " ['home', 'family', 'life'],\n",
       " ['high', 'top'],\n",
       " ['#,###', '##,###', '###,###'],\n",
       " ['State'],\n",
       " ['####', 'after', 'before', 'during', 'since', 'early', 'until'],\n",
       " ['win', 'lead'],\n",
       " ['We', 'They'],\n",
       " ['first', 'second', 'third', '##th'],\n",
       " ['help', 'support'],\n",
       " ['sales'],\n",
       " ['AP'],\n",
       " ['He', 'She'],\n",
       " ['police']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN Birch Alg + Clusters\n",
    "def run_birch(x):\n",
    "    from sklearn.cluster import Birch\n",
    "\n",
    "    brc = Birch(n_clusters = 70)\n",
    "    x2 = brc.fit_predict(x)\n",
    "\n",
    "    print(x2)\n",
    "\n",
    "    clusters = [[] for x in range(70)]\n",
    "    for i in range(1, 300):\n",
    "        clusters[x2[i]].append(z[i])\n",
    "\n",
    "    return clusters\n",
    "\n",
    "run_birch(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN storing clusters\n",
    "clusters300 = [['about', 'over', 'into', 'through', 'under', 'between', 'around', 'past'], ['as', 'most', 'well', 'such', 'very', 'long', 'too'], ['found', 'report', 'show'], ['#.#', '#.##', '##.#', 'per'], ['will', 'would', 'can', 'could', 'should', 'may', 'expected'], ['team', 'against', 'group', 'former', 'case'], ['in', 'for', 'on', 'with', 'the', 'from', 'by', 'an', 'who', 'its', 'which', 'also', 'new', 'while', 'including', 'part', 'own', 'called'], ['more', 'than', 'much', 'lot', 'little'], ['said', 'says', 'told', 'added', 'according'], ['is', 'was', 'be', 'are', 'were', 'being'], ['game', 'play', 'games', 'players'], ['get', 'work', 'go', 'got', 'run', 'come', 'put', 'came', 'went'], ['people', 'school', 'children'], ['right', 'left', 'hit'], ['state', 'government', 'public', 'city', 'area', 'local'], ['year', 'season', 'week', 'month'], ['old', 'man'], ['you', 'your', 'You'], ['have', 'has', 'had', 'been', \"'ve\", 'never'], ['officials', 'members'], ['$', 'money'], ['program', 'system'], ['good', 'best', 'better', 'great', 'big'], ['he', 'his', 'her', 'she', 'him'], ['I', 'my', 'us', 'me'], ['years', 'months', 'days', 'ago'], ['based', '&'], ['their', 'they', 'we', 'our', 'them'], ['U.S.', 'world', 'country', 'American'], ['at', 'time', 'day', 'end', 'today', 'set', 'place', 'start'], ['The', 'It', 'A', 'This', 'That'], ['am', 'pm'], ['points', 'point'], ['use', 'used'], [\"'re\", \"'m\"], ['quarter', 'half'], ['#-#', '##-##'], ['or', 'no', 'any', 'without'], ['But', 'If', 'And', 'As'], ['all', 'two', 'other', 'some', 'three', 'many', 'those', 'four', 'five', 'these', 'both', 'six', 'number', 'few'], ['In', 'For'], ['information'], ['think', 'see', 'say', 'know', 'really'], ['up', 'out', 'back', 'off', 'down'], ['percent', '%'], ['this', 'last', 'next', 'another', 'same', 'each', 'every'], ['million', 'billion'], ['By'], ['take', 'took'], ['company', 'business', 'market', 'companies'], ['Friday', 'Tuesday', 'Monday', 'night', 'Thursday', 'Wednesday', 'Saturday', 'Sunday'], ['not', 'do', 'did', 'does', 'want', 'need'], ['that', 'it', 'but', 'one', 'when', 'there', 'just', 'what', 'so', 'like', 'if', 'only', 'because', 'now', 'where', 'going', 'way', 'how', 'then', 'still', 'even', 'here'], ['made', 'make'], ['services', 'service'], ['##', '###', '1', '2', '3', '5'], ['There'], ['home', 'family', 'life'], ['high', 'top'], ['#,###', '##,###', '###,###'], ['State'], ['####', 'after', 'before', 'during', 'since', 'early', 'until'], ['win', 'lead'], ['We', 'They'], ['first', 'second', 'third', '##th'], ['help', 'support'], ['sales'], ['AP'], ['He', 'She'], ['police']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe just like\n",
    "#add in 'new element' and if adds into another cluster with minimum distance < x\n",
    "#then allow it?\n",
    "#this may not be necessary; all that has to be done to check is like to confirm that faraway points still faraway\n",
    "#only really an issue if in closest cluster though :) - because if it's closest, then it'll ruin the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63  0 55  0  0 68  0  0 50 16  0  0 72  0  0  0  0 71 35  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 47  0  0  0  0  0  0  0  0  0  0  0  0  0  0 61  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 59  0  0  4  0  0  0\n",
      "  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0\n",
      "  0  0  0  0  2  0  1 62  0  0  0 20  0 20  0  3  0  0  1 16  0  0 67  0\n",
      "  0 56  0 10 45  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 53  0  0\n",
      " 79  0  0  0  0 51  0  0  0  0 43  0  0  5  0  0  2  0 16 44  0  0  0  0\n",
      " 77  0  0 48  1 60  0 66  0  0 49 73 27 33 19  0  0  0 57 65 83  0  0 46\n",
      " 75  0  0 32 76  0  5  0  0  0 80  0  7  0  0 21 64 58  0 78  0 52  0  0\n",
      "  0  0  0 69 23  0  0 40  0  0  0  0  0 81  0 24  0  7  0  0  0 25  0  1\n",
      " 29 28  0 54 42  0 38 70  0 30 74 26  0  0  0  0  0 22 36  9 10 13  0  0\n",
      " 16  0 11  0  3 82  4 37 18 39 40  0  0 82  0  0 17  0 31 19 41  2  0 14\n",
      " 12  0 34  8  0  0  6 15  0  0  1  0]\n",
      "[['in', 'that', 'is', '##', 'The', 'was', 'the', 'not', 'as', 'it', 'be', 'are', 'I', 'have', 'he', 'will', 'has', '####', 'his', 'an', 'this', 'or', 'their', 'they', 'but', '$', 'had', 'year', 'were', 'we', 'more', '###', 'up', 'been', 'you', 'its', 'one', 'would', 'which', 'out', 'can', 'It', 'all', 'also', 'two', 'after', 'first', 'He', 'do', 'time', 'than', 'when', 'We', 'over', 'last', 'other', 'her', 'into', 'In', 'our', 'there', 'A', 'she', 'could', 'just', 'years', 'some', 'three', 'million', 'them', 'what', 'But', 'so', 'no', 'like', 'if', 'only', 'percent', 'get', 'did', 'him', 'back', 'because', 'now', '#.#', 'before', 'any', 'off', 'This', 'most', 'through', 'second', 'well', 'day', 'week', 'where', 'down', 'being', 'your', 'going', 'my', 'good', 'They', \"'re\", 'should', 'many', 'way', 'those', 'four', 'during', 'such', 'may', 'very', 'how', 'since', 'take', 'including', 'then', '%', 'next', '#,###', 'much', 'still', 'go', 'think', 'even', '#.##', 'see', 'say', 'five', 'us', '1', 'these', 'If', 'And', 'me', '##,###', 'That', 'know', 'does', 'both', 'There', 'want', \"'ve\", 'got', 'third', 'another', 'need', '2', 'best', 'quarter', 'today', '##.#', 'Friday', 'month', 'billion', 'Tuesday', 'come', 'Monday', 'She', 'night', 'six', 'Thursday', '###,###', 'Wednesday', 'here', 'You', 'really', 'As', '3', 'lot', \"'m\", 'put', 'half', 'months', 'am', 'Saturday', 'too', 'better', 'days', 'came', 'past', 'took', 'expected', 'great', 'pm', 'big', 'few', 'per', 'Sunday', 'little', 'ago', 'never', '5', 'until', 'went', '##th'], ['game', 'team', 'season', 'play', 'games', 'players'], ['company', 'business', 'companies'], ['state', 'State'], ['people', 'children'], ['world', 'country'], ['without'], ['use', 'used'], ['according'], ['U.S.', 'American'], ['#-#', '##-##'], ['called'], ['case'], ['show'], ['&'], ['sales'], ['said', 'says', 'told', 'added'], ['members'], ['area'], ['points', 'point'], ['made', 'make'], ['same'], ['lead'], ['place'], ['found'], ['money'], ['man'], ['long'], ['program'], ['support'], ['former'], ['early'], ['school'], ['information'], ['local'], ['by'], ['life'], ['hit'], ['number'], ['system'], ['each', 'every'], ['start'], ['family'], ['old'], ['under'], ['home'], ['based'], ['who'], ['between'], ['end'], ['with'], ['By'], ['top'], ['work'], ['report'], ['for'], ['government'], ['part'], ['run'], ['new'], ['help'], ['about'], ['against'], [], ['public'], ['around'], ['market'], ['while'], ['on'], ['city'], ['officials'], ['from'], ['at'], ['AP'], ['own'], ['For'], ['left'], ['right'], ['set'], ['high'], ['win'], ['group'], ['services', 'service'], ['police']]\n"
     ]
    }
   ],
   "source": [
    "#Agglomerative Clustering alg\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "x2 = AgglomerativeClustering(n_clusters = None, affinity='cosine', linkage = 'single', distance_threshold=0.5).fit(x)\n",
    "x2 = x2.labels_\n",
    "\n",
    "print(x2)\n",
    "\n",
    "clusters = [[] for x in range(300)]\n",
    "for i in range(1, 300):\n",
    "    clusters[x2[i]].append(z[i])\n",
    "while clusters[-1] == []:\n",
    "    clusters.pop()\n",
    "\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'This', 'That'], ['two', 'three', 'four', 'five', 'six'], ['#,###', '##,###', '###,###'], ['##', '1', '2', '3', '5'], ['Friday', 'Tuesday', 'Monday', 'Thursday', 'Wednesday', 'Saturday', 'Sunday'], ['in', 'for', 'that', 'is', 'on', 'The', 'with', 'said', 'was', 'the', 'at', 'not', 'as', 'it', 'be', 'from', 'by', 'are', 'I', 'have', 'he', 'will', 'has', '####', 'his', 'an', 'this', 'or', 'their', 'who', 'they', 'but', '$', 'had', 'year', 'were', 'we', 'more', '###', 'up', 'been', 'you', 'its', 'one', 'about', 'would', 'which', 'out', 'can', 'all', 'also', 'after', 'first', 'He', 'do', 'time', 'than', 'when', 'We', 'over', 'last', 'new', 'other', 'her', 'people', 'into', 'In', 'our', 'there', 'A', 'she', 'could', 'just', 'years', 'some', 'U.S.', 'million', 'them', 'what', 'But', 'so', 'no', 'like', 'if', 'only', 'percent', 'get', 'did', 'him', 'game', 'back', 'because', 'now', '#.#', 'before', 'company', 'any', 'team', 'against', 'off', 'most', 'made', 'through', 'make', 'second', 'state', 'well', 'day', 'season', 'says', 'week', 'where', 'while', 'down', 'being', 'government', 'your', '#-#', 'home', 'going', 'my', 'good', 'They', \"'re\", 'should', 'many', 'way', 'those', 'during', 'such', 'may', 'very', 'how', 'since', 'work', 'take', 'including', 'high', 'then', '%', 'next', 'By', 'much', 'still', 'go', 'think', 'old', 'even', '#.##', 'world', 'see', 'say', 'business', 'told', 'under', 'us', 'these', 'If', 'right', 'And', 'me', 'between', 'play', 'help', 'market', 'know', 'end', 'AP', 'long', 'information', 'points', 'does', 'both', 'There', 'part', 'around', 'police', 'want', \"'ve\", 'based', 'For', 'got', 'third', 'school', 'left', 'another', 'country', 'need', 'best', 'win', 'quarter', 'use', 'today', '##.#', 'same', 'public', 'run', 'set', 'month', 'top', 'billion', 'come', 'She', 'city', 'place', 'night', 'each', 'here', 'You', 'group', 'really', 'found', 'As', 'used', 'lot', \"'m\", 'money', 'put', 'games', 'support', 'program', 'half', 'report', 'family', 'months', 'number', 'officials', 'am', 'former', 'own', 'man', 'too', 'better', 'days', 'came', 'lead', 'life', 'American', '##-##', 'show', 'past', 'took', 'added', 'expected', 'called', 'great', 'State', 'services', 'children', 'hit', 'area', 'system', 'every', 'pm', 'big', 'service', 'few', 'per', 'members', 'early', 'point', 'start', 'companies', 'little', '&', 'case', 'ago', 'local', 'according', 'never', 'without', 'sales', 'until', 'went', 'players', '##th']]\n"
     ]
    }
   ],
   "source": [
    "#DBSCAN Clustering Alg\n",
    "from sklearn.cluster import DBSCAN\n",
    "x2 = DBSCAN(eps=0.2, min_samples=3, metric='cosine').fit(x)\n",
    "x2 = x2.labels_\n",
    "\n",
    "clusters = [[] for x in range(300)]\n",
    "for i in range(1, 300):\n",
    "    clusters[x2[i]].append(z[i])\n",
    "\n",
    "clusters = list(filter(None, clusters))\n",
    "\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['</s>', 'our', 'on', 'no', 'company', 'By', 'we', 'us', 'your', 'my'], ['in', 'where', 'the', 'In', 'during', 'at', 'when', 'here', 'then', 'what'], ['for', 'in', 'For', 'as', 'but', 'the', 'where', 'In', 'during', 'at'], ['that', 'it', 'not', 'if', 'but', 'what', 'just', 'It', 'really', 'do'], ['is', 'was', 'are', \"'re\", \"'m\", 'now', 'had', 'were', 'been', 'came'], ['##', '5', '3', '2', '###', 'six', '1', '#,###', '##,###', '#.#'], ['The', 'This', 'That', 'A', 'It', 'which', 'But', 'And', 'If', 'that'], ['with', 'between', 'in', 'had', 'while', 'by', 'over', 'both', 'through', 'where'], ['said', 'says', 'told', 'added', 'according', 'think', 'does', 'know', 'also', 'had'], ['be', 'being', 'are', 'have', 'should', 'been', 'was', 'were', \"'re\", 'these'], ['from', 'in', 'after', 'where', 'through', 'the', 'In', 'during', 'at', 'before'], ['I', \"'m\", 'my', 'me', 'we', 'you', \"'re\", 'am', 'really', 'your'], ['he', 'He', 'him', 'his', 'she', 'I', 'She', 'They', 'But', 'but'], ['will', 'can', 'would', 'should', 'could', 'may', 'want', 'did', 'do', 'need'], ['has', 'had', 'been', 'have', \"'ve\", 'since', 'was', 'were', 'they', 'we'], ['####', 'since', '##', '1', 'year', 'last', 'been', 'has', 'after', '5'], ['an', 'another', 'this', 'the', 'A', 'was', 'one', 'next', 'every', 'that'], ['or', 'any', 'your', 'can', 'you', 'if', 'no', 'not', 'never', 'my'], ['their', 'they', 'them', 'our', 'its', 'your', 'we', 'They', 'do', 'us'], ['who', 'He', 'also', 'he', 'former', 'him', 'She', 'They', 'But', 'but'], ['$', 'million', '###,###', '#.##', '#.#', '##,###', 'billion', '#,###', '###', '##.#'], ['more', 'than', 'most', 'little', 'better', 'about', 'much', 'even', 'one', 'very'], ['up', 'down', 'out', 'off', 'around', 'back', 'into', 'on', 'over', 'where'], ['all', 'these', 'those', 'other', 'some', 'both', 'many', 'are', 'including', 'such'], ['two', 'three', 'four', 'five', 'six', 'few', '##', 'some', 'many', '5'], ['first', 'second', 'third', 'last', '##th', 'next', 'half', 'three', 'six', 'ago'], ['time', 'day', 'days', 'when', 'months', 'year', 'week', 'month', 'night', 'years'], ['We', 'They', 'we', 'You', 'If', 'I', 'But', 'they', 'And', 'our'], ['new', 'next', 'will', 'another', 'the', 'own', 'last', 'first', 'this', 'can'], ['her', 'she', 'his', 'She', 'my', 'him', 'he', 'I', 'He', 'their'], ['people', 'children', 'those', 'them', 'us', 'just', 'family', 'school', 'these', 'other'], ['there', 'There', 'no', 'here', 'going', 'we', 'But', 'And', 'It', 'If'], ['U.S.', 'American', 'world', 'country', 'billion', 'government', 'AP', 'our', 'most', 'one'], ['so', 'too', 'but', 'because', 'really', 'very', 'not', 'But', 'It', 'if'], ['like', 'really', 'think', 'just', 'do', 'want', 'so', 'I', 'very', 'know'], ['only', 'one', 'just', 'not', 'but', 'even', 'two', 'three', 'five', 'four'], ['percent', '%', '#.#', '##.#', 'million', 'billion', '#.##', '###,###', '###', '##'], ['get', 'got', 'go', 'come', 'do', 'just', 'came', 'went', 'had', \"'ve\"], ['game', 'games', 'play', 'season', 'players', 'team', 'points', 'win', 'go', 'year'], ['against', 'game', 'win', 'in', '#-#', 'case', 'games', 'play', 'season', 'players'], ['made', 'make', 'came', 'had', 'no', 'did', 'get', 'do', 'if', 'come'], ['state', 'State', 'government', 'country', 'city', 'local', 'U.S.', 'Saturday', 'former', 'officials'], ['well', 'as', 'good', 'much', 'such', 'better', 'As', 'so', 'but', 'great'], ['home', 'family', 'back', 'game', 'off', 'when', 'children', 'life', 'own', 'people'], ['way', 'how', 'going', 'it', 'really', 'so', 'what', 'if', 'want', 'do'], ['work', 'do', 'go', 'get', 'come', 'so', 'want', 'know', 'not', 'did'], ['take', 'took', 'go', 'put', 'get', 'come', 'went', 'came', 'got', 'had'], ['high', 'top', 'down', 'school', 'well', 'up', 'best', 'second', 'third', 'one'], ['still', 'now', 'but', 'even', 'just', 'only', 'right', 'is', \"'re\", 'because'], ['old', 'man', 'ago', 'year', 'last', 'who', 'him', 'he', 'police', 'his'], ['see', 'know', 'think', 'do', 'get', 'say', 'really', 'want', 'not', 'did'], ['business', 'company', 'companies', 'market', 'sales', 'services', 'its', 'government', 'quarter', 'percent'], ['under', 'put', 'on', 'into', 'without', 'the', 'come', 'go', 'take', 'get'], ['help', 'need', 'support', 'better', 'can', 'do', 'want', 'should', 'services', 'money'], ['end', 'start', 'until', 'point', 'next', 'back', 'go', 'early', 'run', 'before'], ['long', 'many', 'well', 'much', 'just', 'few', 'some', 'those', 'these', 'all'], ['information', 'services', 'report', 'money', 'For', 'any', 'service', 'companies', 'business', 'support'], ['part', 'this', 'because', 'the', 'also', 'really', 'another', 'that', 'last', 'it'], ['based', 'company', 'its', 'in', 'from', 'according', 'companies', 'business', 'market', 'sales'], ['left', 'right', 'went', 'took', 'came', 'back', 'now', 'just', 'going', 'do'], ['use', 'used', 'need', 'do', 'take', 'help', 'can', 'could', 'called', 'want'], ['today', 'Thursday', 'Wednesday', 'Monday', 'Tuesday', 'Friday', 'Saturday', 'Sunday'], ['same', 'every', 'each', 'this', 'the', 'only', 'another', 'one', 'all', 'other'], ['public', 'government', 'local', 'people', 'state', 'city', 'country', 'billion', 'companies', 'area'], ['set', 'put', 'come', 'start', 'the', 'for', 'go', 'take', 'get', 'came'], ['place', 'where', 'time', 'in', 'lead', 'the', 'when', 'here', 'then', 'what'], ['group', 'team', 'who', 'members', 'company', 'program', 'players', 'game', 'season', 'games'], ['found', 'were', 'according', 'say', 'see', 'was', 'are', 'had', 'been', 'have'], ['lot', 'some', 'little', 'really', 'much', 'great', 'many', 'few', 'those', 'these'], ['number', 'many', 'two', 'these', 'those', 'four', 'some', 'few', 'all', 'three'], ['##-##', '#-#', '##', '3', '##.#', '5', 'win', 'game', 'points', 'second'], ['show', 'see', 'say', 'know', 'so', 'program', 'think', 'do', 'get', 'not'], ['past', 'over', 'last', 'ago', 'years', 'have', 'around', 'between', 'off', 'first'], ['expected', 'will', 'could', 'next', 'would', 'going', 'can', 'should', 'may', 'last'], ['hit', 'off', 'left', 'run', 'went', 'came', 'down', 'out', 'back', 'up'], ['system', 'program', 'service', 'services', 'state', 'way', 'group', 'show', 'business', 'work'], ['pm', 'am', 'Saturday', 'Sunday', 'Friday', 'Thursday', \"'m\", 'I', \"'re\", 'is'], ['big', 'great', 'good', 'lot', 'really', 'little', 'best', 'better', 'some', 'much'], ['per', '#.##', '##.#', '#.#', '$', '###', '5', 'percent', '##', '%'], ['&', 'By', '#.##', '3', 'A', '2', 'by', 'AP', 'In', 'For']]\n"
     ]
    }
   ],
   "source": [
    "#bash list of clusters\n",
    "\n",
    "def cluster300():\n",
    "    #swap to snake_case\n",
    "    touched_points = []\n",
    "    clusters = []\n",
    "\n",
    "    for i in range (300):\n",
    "        if z[i] in touched_points:\n",
    "            continue\n",
    "        curr_cluster = make_cluster(z[i])\n",
    "        clusters.append(curr_cluster)\n",
    "        # print(\"check2\")\n",
    "        # print(curr_cluster)\n",
    "        touched_points.extend(curr_cluster)\n",
    "        # print(touched_points)\n",
    "\n",
    "    #call other function to recursively add elements \n",
    "    #curr cluster = list\n",
    "    #append to total list of clusters\n",
    "    print(clusters)\n",
    "    return clusters\n",
    "\n",
    "#z = wv\n",
    "def make_cluster(word):\n",
    "    curr_cluster=[]\n",
    "    visited = []\n",
    "    queue = []\n",
    "    queue.append(word)\n",
    "    visited.append(word)\n",
    "\n",
    "    z2 = z.copy()\n",
    "\n",
    "    while len(curr_cluster) < 10 and queue:\n",
    "        curr_word = queue.pop(0)\n",
    "        if curr_word in curr_cluster:\n",
    "            break\n",
    "\n",
    "        curr_cluster.append(curr_word)\n",
    "\n",
    "        z2.remove(curr_word)\n",
    "\n",
    "        for i in range(5):\n",
    "\n",
    "            closest_word = wv.most_similar_to_given(curr_word, z2)\n",
    "            if (closest_word in z2):\n",
    "                z2.remove(closest_word)\n",
    "\n",
    "            if not (closest_word in visited):\n",
    "                queue.append(closest_word)\n",
    "                visited.append(closest_word)\n",
    "\n",
    "        z2 = z.copy()\n",
    "\n",
    "    return curr_cluster\n",
    "\n",
    "clusters = cluster300()\n",
    "\n",
    "#ALL THE METHODS ARE BASICALLY JUST BASH EVERYTHING, THEREFORE CREATE OWN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine distances for future clusters? Unfinished\n",
    "\n",
    "\n",
    "\n",
    "def check_clusters(clusters, data):\n",
    "    for i in range(len(clusters)):\n",
    "        for j in range(len(clusters[i])):\n",
    "            if (1-cosine(x.data, >): #need to fix lol\n",
    "                print\n",
    "        \n",
    "x = dataAnalysis(7, data)\n",
    "check_clusters(clusters, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting algo\n",
    "\n",
    "#t-ttttttt sorting fails because two very far away points could still be the same distance from middle point :(\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def clusters_ndim (data, words):\n",
    "    dim = len(data[0])\n",
    "    print(dim)\n",
    "    #basically what I want to do is sort the words for 'closeness' to the mean vector (origin) and then \n",
    "    #first save words with data\n",
    "    data = makeKeyVec(data, words)\n",
    "    #note that distance can be defined in any way sooo\n",
    "\n",
    "    #far away things can still be the same distance from the rand\n",
    "    rand = [1]*dim\n",
    "    #need to find 'mean vector' - pca can't already do the mean vector stuff for me then :()\n",
    "    data.sort(key = lambda x: (1-cosine(x.data, rand)))\n",
    "    #need to add the words to the data, but ignore it in the vectors themselves\n",
    "    #check with the words directly above and directly below (maybe x 2? and same not in cluster method)\n",
    "    #then establish clusters\n",
    "    for i in range(dim):\n",
    "        data[i].prints()\n",
    "\n",
    "clusters_ndim(x, z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nishka\\OneDrive\\Java\\PCA\\PCAGit\\PCA-semantle\\SemantlePCA.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000020?line=13'>14</a>\u001b[0m         keyvecs\u001b[39m.\u001b[39mappend(curr_keyvec)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000020?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m keyvecs\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000020?line=16'>17</a>\u001b[0m x2 \u001b[39m=\u001b[39m makeKeyVec(x, z)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "class KeyVec:\n",
    "    #figure out if cosine distance still works on pca\n",
    "    def __init__(self, data2, word2):\n",
    "        self.data = data2\n",
    "        self.word = word2\n",
    "\n",
    "    def prints(self):\n",
    "        print(str(self.word) + \": \" + str(self.data))\n",
    "\n",
    "def makeKeyVec(data, words):\n",
    "    keyvecs = []\n",
    "    for i in range(len(data)):\n",
    "        curr_keyvec = KeyVec(data[i], words)\n",
    "        keyvecs.append(curr_keyvec)\n",
    "    return keyvecs\n",
    "\n",
    "x2 = makeKeyVec(x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTransform = np.asarray([\n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [],  \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "])\n",
    "\n",
    "maxNum = 0\n",
    "\n",
    "def dataAnalysis(dim, data):\n",
    "\n",
    "    global maxTransform, maxNum\n",
    "    \n",
    "    if maxNum > dim:\n",
    "        maxTransform2 = np.transpose(maxTransform)\n",
    "        maxTransform2 = maxTransform2[:dim]\n",
    "        maxTransform2 = np.transpose(maxTransform2)\n",
    "        return maxTransform2\n",
    "\n",
    "    else:\n",
    "        newDim = dim - maxNum\n",
    "\n",
    "        arr2 = data[ : , 300-newDim : data[0].__len__() ]\n",
    "        print(newDim)\n",
    "        print(len(data[0]))\n",
    "\n",
    "        pca = PCA(n_components = newDim)\n",
    "        pca.fit(arr2)\n",
    "        arr2 = np.asarray(pca.transform(arr2))\n",
    "\n",
    "        np.transpose(maxTransform)\n",
    "        np.transpose(arr2)\n",
    "        maxTransform = np.append(maxTransform, arr2, 1)\n",
    "        np.transpose(maxTransform)\n",
    "\n",
    "        # print(type(maxTransform))\n",
    "        # print(type(arr2))\n",
    "        # print(\"Max Transform # of vectors \" + str(maxTransform.__len__()))\n",
    "        # print(\"Max Transform Size of vectors \" + str(maxTransform[0].__len__()))\n",
    "\n",
    "        maxNum = dim\n",
    "        return maxTransform\n",
    "\n",
    "# x = np.array([[1, 2, 3, 4, 5], [2, 1, 3, 3, 1], [2, 4, 1, 2, 3], [4, 3, 2, 1, 4]])\n",
    "# print(dataAnalysis(2, x))\n",
    "# # print(dataAnalysis(3, x))\n",
    "\n",
    "#dataAnalysis(300, x)\n",
    "\n",
    "#perhaps describe how to do it with gradient function and the like to pick a random dimension\n",
    "#instead of going down one dimension at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(dim, data):\n",
    "    #data = np.array(data)\n",
    "    if dim == 3 or dim == 2:\n",
    "        fig = plt.figure(figsize=(15, 15))\n",
    "        plt.clf()\n",
    "\n",
    "        if dim == 3:\n",
    "            ax = fig.add_subplot(projection = \"3d\") #this is rectilinear, 3d, etc. projection= \"3d\"\n",
    "        elif dim == 2:\n",
    "            ax = fig.add_subplot(projection = \"rectilinear\")\n",
    "        \n",
    "        ax.set_position([0, 0, 0.95, 1])\n",
    "        plt.cla()\n",
    "           \n",
    "        if dim == 3:\n",
    "            ax.scatter(data[:, 0], data[:, 1], data[:, 2])\n",
    "            #not allowed to do it in 3 dimensions; b/c they expect a point of size two in annotation function\n",
    "            # for i in range (300):\n",
    "            #     ax.annotate(f.readline(), (data[i, 0], data[i, 1], data[i, 2]))\n",
    "            #     f.close()\n",
    "        elif dim == 2:\n",
    "            ax.scatter(data[:, 0], data[:, 1])\n",
    "            # plt.ylim(-1, 3)\n",
    "            # plt.xlim(-1.5, 2)\n",
    "            for i in range (300):\n",
    "                ax.annotate(z[i], (data[i, 0], data[i, 1]))\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(\"Too many/too few dimensions to visualize\")\n",
    "        ans = \"y\"\n",
    "        #ans = input(\"Proceed with 2-D? y/n\"\n",
    "        if ans == \"y\":\n",
    "            fig = plt.figure(figsize=(15 * dim, 15 * dim))\n",
    "            fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "            for i in range(dim):\n",
    "                for j in range(i+1, dim):\n",
    "                    #dim, dim-1, (i+1)*(j+1\n",
    "                    ax = fig.add_subplot()\n",
    "                    ax.scatter(data[:, i], data[:, j])\n",
    "\n",
    "                    #with open(\"words.txt\", \"r\") as f:\n",
    "                        #ax.set_position([0, 0, 20.95, 20])\n",
    "                    for k in range (300):\n",
    "                        ax.annotate(z[i], (data[k, i], data[k, j]))\n",
    "                        #f.close()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n",
      "</s> public based local service services \n",
      "in with from during for over after before \n",
      "that as how because it but what if \n",
      "is was be been am an had \n",
      "on through at into from off \n",
      "## 3 ### 2 5 1 \n",
      "The A This As In It \n",
      "said told says added according called made found \n",
      "the in new its another </s> with from \n",
      "not if does too even do how \n",
      "by in while with By from \n",
      "are were be these 're been being those \n",
      "I me 'm my really we think know \n",
      "have been were many those these has \n",
      "he she him never me her even I \n",
      "will should can may would could \n",
      "#### last third ## ##th 1 first ago \n",
      "his her him my she man he me \n",
      "this every another same next an \n",
      "or without any can your may no \n",
      "their them own other those its they some \n",
      "who he case school He that she him \n",
      "$ million billion percent ###,### #.# \n",
      "year month months week years days \n",
      "more than most about many still \n",
      "up down out off back into \n",
      "you your You I do us me 'm \n",
      "one only long two three four \n",
      "which as The As He that \n",
      "all these those many some both \n",
      "also as that both well most \n",
      "time days years months day past \n",
      "when until before then after early \n",
      "We They You And we If you But \n",
      "people children family life school police \n",
      "our us we your you my \n",
      "there lot little no here some \n",
      "just really think little 've I \n",
      "U.S. American new state country government city \n",
      "so because too how really know but still \n",
      "like good better great really see \n",
      "get go come going do got want \n",
      "did would could does not want \n",
      "game games players season play team win run \n",
      "now see about still time past know think \n",
      "company companies market business sales services \n",
      "against lead early hit game #-# run win \n",
      "make take come put made get \n",
      "second third first ##th last #-# ago since \n",
      "where around out as when how through down \n",
      "home school place lead left against children family \n",
      "way how what too it better \n",
      "such other some both these many \n",
      "very really so think here well \n",
      "work support come show help information services program \n",
      "including from other with between number in \n",
      "high top also with market between big best \n",
      "% percent million billion $ #.# \n",
      "#,### ##,### ###,### ### #.# ## ##.# #.## \n",
      "much little too even better not no still \n",
      "old man former another left ago American U.S. \n",
      "world country American U.S. new best \n",
      "say know still says money too think do \n",
      "five six four three two ## \n",
      "under into from over through between \n",
      "right like back way going good \n",
      "That It This And But As \n",
      "end start next back third first \n",
      "AP American U.S. State former officials \n",
      "points point #-# game games ##-## \n",
      "There And It That But This \n",
      "part business program called new support market companies \n",
      "For In This As And That The It \n",
      "need want can do should will \n",
      "quarter half ##.# percent % sales #.# #.## \n",
      "use used help make such support take come \n",
      "today Friday Thursday Monday Wednesday Tuesday \n",
      "set put take run lead took \n",
      "She He she They It her he \n",
      "night Saturday Sunday Friday Tuesday Thursday \n",
      "each every one three five or same this \n",
      "group area country government report local world state \n",
      "came went took got put made take come \n",
      "expected will could would may month should can \n",
      "system services program service company companies \n",
      "pm Saturday Sunday night am at \n",
      "few two three four many some \n",
      "per #.## #.# ##.# $ ### \n",
      "members public family children service area local information \n",
      "& based market By services </s> \n",
      "</s> public based local service services \n",
      "in with from during for over after before \n",
      "that as how because it but what if \n",
      "is was be been am an had \n",
      "on through at into from off \n",
      "## 3 ### 2 5 1 \n",
      "The A This As In It \n",
      "said told says added according called made found \n",
      "the in new its another </s> with from \n",
      "not if does too even do how \n",
      "by in while with By from \n",
      "are were be these 're been being those \n",
      "I me 'm my really we think know \n",
      "have been were many those these has \n",
      "he she him never me her even I \n",
      "will should can may would could \n",
      "#### last third ## ##th 1 first ago \n",
      "his her him my she man he me \n",
      "this every another same next an \n",
      "or without any can your may no \n",
      "their them own other those its they some \n",
      "who he case school He that she him \n",
      "$ million billion percent ###,### #.# \n",
      "year month months week years days \n",
      "more than most about many still \n",
      "up down out off back into \n",
      "you your You I do us me 'm \n",
      "one only long two three four \n",
      "which as The As He that \n",
      "all these those many some both \n",
      "also as that both well most \n",
      "time days years months day past \n",
      "when until before then after early \n",
      "We They You And we If you But \n",
      "people children family life school police \n",
      "our us we your you my \n",
      "there lot little no here some \n",
      "just really think little 've I \n",
      "U.S. American new state country government city \n",
      "so because too how really know but still \n",
      "like good better great really see \n",
      "get go come going do got want \n",
      "did would could does not want \n",
      "game games players season play team win run \n",
      "now see about still time past know think \n",
      "company companies market business sales services \n",
      "against lead early hit game #-# run win \n",
      "make take come put made get \n",
      "second third first ##th last #-# ago since \n",
      "where around out as when how through down \n",
      "home school place lead left against children family \n",
      "way how what too it better \n",
      "such other some both these many \n",
      "very really so think here well \n",
      "work support come show help information services program \n",
      "including from other with between number in \n",
      "high top also with market between big best \n",
      "% percent million billion $ #.# \n",
      "#,### ##,### ###,### ### #.# ## ##.# #.## \n",
      "much little too even better not no still \n",
      "old man former another left ago American U.S. \n",
      "world country American U.S. new best \n",
      "say know still says money too think do \n",
      "five six four three two ## \n",
      "under into from over through between \n",
      "right like back way going good \n",
      "That It This And But As \n",
      "end start next back third first \n",
      "AP American U.S. State former officials \n",
      "points point #-# game games ##-## \n",
      "There And It That But This \n",
      "part business program called new support market companies \n",
      "For In This As And That The It \n",
      "need want can do should will \n",
      "quarter half ##.# percent % sales #.# #.## \n",
      "use used help make such support take come \n",
      "today Friday Thursday Monday Wednesday Tuesday \n",
      "set put take run lead took \n",
      "She He she They It her he \n",
      "night Saturday Sunday Friday Tuesday Thursday \n",
      "each every one three five or same this \n",
      "group area country government report local world state \n",
      "came went took got put made take come \n",
      "expected will could would may month should can \n",
      "system services program service company companies \n",
      "pm Saturday Sunday night am at \n",
      "few two three four many some \n",
      "per #.## #.# ##.# $ ### \n",
      "members public family children service area local information \n",
      "& based market By services </s> \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def playable(dim, data):\n",
    "\n",
    "    y = dataAnalysis(dim, data)\n",
    "    y2 = cluster_gen(y)\n",
    "\n",
    "    for i in range(len(y2)):\n",
    "        for j in range(len(y2[i])):\n",
    "            print(z[y2[i][j]], end = \" \")\n",
    "        if not (len(y2[i]) == 0):\n",
    "            print()\n",
    "\n",
    "    if dim >= 2: \n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "print(len(x))\n",
    "print(len(x[i]))\n",
    "playable(19, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDim(data, start=1, end=300):\n",
    "    print(start)\n",
    "    print(end)\n",
    "    if start >= end:\n",
    "        return end\n",
    "    else:\n",
    "        mid = (start + end)//2\n",
    "        if not playable(mid, data):\n",
    "            return findDim(data, mid+1, end)\n",
    "        else:\n",
    "            return findDim(data, start, mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.array([[1, 2, 3, 4, 5], [2, 1, 3, 3, 1], [2, 4, 1, 2, 3], [4, 3, 2, 1, 4]])\n",
    "#OF COURSE IT WON'T WORK - IT'S A MAX OF 4 DIMENSIONS AND YOU'RE RUNNING 150 ON IT!\n",
    "print(len(x))\n",
    "#dataAnalysis(150, x)\n",
    "m = findDim(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f07e8f80a78d731554c2ce7ed8433d0fa1e00c779c09dd1ced00d3c38371f3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
