{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Exists\n",
      "KeyedVectors<vector_size=300, 3000000 keys>\n"
     ]
    }
   ],
   "source": [
    "#general imports\n",
    "\n",
    "import os\n",
    "\n",
    "if not {\"word2vec.model\", \"words.txt\"}.issubset(set(os.listdir())):\n",
    "\n",
    "    print(\"Beginning download\")\n",
    "\n",
    "    import gensim.downloader\n",
    "\n",
    "    print(\"imported\")\n",
    "\n",
    "    wv = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "\n",
    "    print(\"loaded\")\n",
    "\n",
    "    wv.save(\"./word2vec.model\")\n",
    "\n",
    "    print(\"saved\")\n",
    "    \n",
    "    f = open(\"words.txt\", \"x\")\n",
    "    for index, word in enumerate(wv.index_to_key):\n",
    "        if index == 300:\n",
    "            break\n",
    "        f.write(word)\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "print(\"File Exists\")\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load(\"word2vec.model\", mmap=\"r\")\n",
    "print(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates x\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = []\n",
    "for i in range(300):\n",
    "    x.append(wv[i])\n",
    "    # if (i == 100000):\n",
    "    #     print(\"hi\")\n",
    "    # if (i == 1000000):\n",
    "    #     print(\"hi\")\n",
    "x = np.asarray(x)\n",
    "\n",
    "# x = np.array([])\n",
    "# for i in range(300):\n",
    "#     x = np.append(x, wv[i])\n",
    "#     print(x[i][0], end = \" \")\n",
    "# print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generates array z (words)\n",
    "z = []\n",
    "with open(\"words.txt\", 'r') as f:\n",
    "    for i in range(300):\n",
    "        s = f.readline()\n",
    "        s = s[:len(s)-1]\n",
    "        z.append(s)\n",
    "#print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.KeyedVectors'>\n",
      "KeyedVectors<vector_size=300, 3000000 keys>\n",
      "KeyedVectors<vector_size=300, 300 keys>\n"
     ]
    }
   ],
   "source": [
    "#convert keyed vectors to smaller length\n",
    "\n",
    "print(type(wv))\n",
    "print(wv)\n",
    "iterator_for_words = (key for key in z)\n",
    "wv = wv.vectors_for_all(iterator_for_words)\n",
    "print(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29 29 29 12  9 29 55 57 29  8  9 29 14 51  0 12  9 29 29  9 24 37 23  4\n",
      " 37 30 23 18 45 75 27 29 27 12 20 37  7  9 27  3 55 43 37 17 29 12 26  4\n",
      " 29 43  4 77 39 29 39 30 64 68 51 14  3 12 63 26 45 29 39 23 62 16 40 27\n",
      " 12 57 23  4 12  5 39 13 39 46 27 12 78 12 75 12 12 12 44 15 51 23 10 43\n",
      " 12 12  1 30 49 75 72 11 43 77  0 53 16 53 64 52  0 14  7  8  7 12 29 43\n",
      "  9 52 17 36 28 12 24 22 63 70  4 39 12 39 39 30  0  4  0 12 30 38 48 29\n",
      " 58 12 44 45 59 47  3 12 38 42 34 12  1 13 42 42 49 39  8 16 24 55 39 78\n",
      "  6 78 24 26 10 65 59 49 77 42 14 67  0 41 32 51 39 56 18 26 69 51 37 61\n",
      " 40 15 64 73  6 45 13 51 55 22  2 35 33 14  1 45 52 38 50 14  7 58 46 50\n",
      " 38 50 68 76 14 50 39 45 50 59 50 12 17 72 42 25 78 33 55  3 70 20 38 10\n",
      " 65 21 35 71 28  5 39 19 31 11 29 34 50  0 22  5 15  2 28 13 36 25 26 48\n",
      "  8  4 18 22 60 54 62  6 76 21 45 31 22 54 39  1 19 50 30 32 14 49  3 74\n",
      " 11  5 52  8 37 55 75 66 30 15 10 64]\n",
      "[['as', 'most', 'well', 'such', 'very', 'long', 'too'], ['#.#', '#.##', '##.#', 'per'], ['win', 'lead'], ['more', 'than', 'much', 'lot', 'little'], ['will', 'would', 'can', 'could', 'should', 'may', 'expected'], ['years', 'months', 'days', 'ago'], ['right', 'left', 'hit'], ['year', 'season', 'week', 'month'], ['said', 'says', 'told', 'added', 'according'], ['is', 'was', 'be', 'are', 'were', 'being'], ['game', 'play', 'games', 'players'], ['against', 'former', 'case'], ['that', 'it', 'but', 'one', 'when', 'there', 'just', 'what', 'so', 'like', 'if', 'only', 'because', 'now', 'where', 'going', 'way', 'how', 'then', 'still', 'even', 'here'], ['U.S.', 'world', 'country', 'American'], ['at', 'time', 'day', 'end', 'today', 'set', 'place', 'start'], ['get', 'got', 'came', 'went'], ['into', 'through', 'under'], ['you', 'your', 'You'], ['an', 'part', 'called'], ['officials', 'members'], ['$', 'money'], ['program', 'system'], ['good', 'best', 'better', 'great', 'big'], ['he', 'his', 'her', 'she', 'him'], ['I', 'my', 'us', 'me'], ['found', 'show'], ['about', 'over', 'between', 'around', 'past'], ['their', 'they', 'we', 'our', 'them'], ['home', 'family', 'life'], ['in', 'for', 'on', 'with', 'the', 'from', 'by', 'who', 'its', 'which', 'also', 'new', 'while', 'including', 'own'], ['####', 'after', 'before', 'during', 'since', 'early', 'until'], ['am', 'pm'], ['points', 'point'], ['use', 'used'], ['old', 'man'], ['quarter', 'half'], ['#-#', '##-##'], ['have', 'has', 'had', 'been', \"'ve\", 'never'], ['work', 'go', 'run', 'come', 'put'], ['all', 'two', 'other', 'some', 'three', 'many', 'those', 'four', 'five', 'these', 'both', 'six', 'number', 'few'], ['In', 'For'], ['information'], ['think', 'see', 'say', 'know', 'really'], ['up', 'out', 'back', 'off', 'down'], ['percent', '%'], ['this', 'last', 'next', 'another', 'same', 'each', 'every'], ['million', 'billion'], ['By'], ['take', 'took'], ['company', 'business', 'market', 'companies'], ['Friday', 'Tuesday', 'Monday', 'night', 'Thursday', 'Wednesday', 'Saturday', 'Sunday'], ['not', 'do', 'did', 'does', 'want', 'need'], ['state', 'government', 'public', 'local'], ['made', 'make'], ['services', 'service'], ['##', '###', '1', '2', '3', '5'], ['There'], ['The', 'A'], ['high', 'top'], ['#,###', '##,###', '###,###'], ['State'], ['based'], ['people', 'children'], ['We', 'They'], ['first', 'second', 'third', '##th'], ['help', 'support'], ['sales'], ['AP'], ['He', 'She'], ['police'], [\"'re\", \"'m\"], ['report'], ['team', 'group'], ['school'], ['&'], ['or', 'no', 'any', 'without'], ['city', 'area'], ['It', 'This', 'That'], ['But', 'If', 'And', 'As']]\n"
     ]
    }
   ],
   "source": [
    "#Birch Alg + Clusters\n",
    "\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "brc = Birch(n_clusters = 79)\n",
    "x2 = brc.fit_predict(x)\n",
    "\n",
    "print(x2)\n",
    "\n",
    "clusters = [[] for x in range(79)]\n",
    "for i in range(1, 300):\n",
    "    clusters[x2[i]].append(z[i])\n",
    "\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63  0 55  0  0 68  0  0 50 16  0  0 72  0  0  0  0 71 35  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 47  0  0  0  0  0  0  0  0  0  0  0  0  0  0 61  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 59  0  0  4  0  0  0\n",
      "  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0\n",
      "  0  0  0  0  2  0  1 62  0  0  0 20  0 20  0  3  0  0  1 16  0  0 67  0\n",
      "  0 56  0 10 45  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 53  0  0\n",
      " 79  0  0  0  0 51  0  0  0  0 43  0  0  5  0  0  2  0 16 44  0  0  0  0\n",
      " 77  0  0 48  1 60  0 66  0  0 49 73 27 33 19  0  0  0 57 65 83  0  0 46\n",
      " 75  0  0 32 76  0  5  0  0  0 80  0  7  0  0 21 64 58  0 78  0 52  0  0\n",
      "  0  0  0 69 23  0  0 40  0  0  0  0  0 81  0 24  0  7  0  0  0 25  0  1\n",
      " 29 28  0 54 42  0 38 70  0 30 74 26  0  0  0  0  0 22 36  9 10 13  0  0\n",
      " 16  0 11  0  3 82  4 37 18 39 40  0  0 82  0  0 17  0 31 19 41  2  0 14\n",
      " 12  0 34  8  0  0  6 15  0  0  1  0]\n",
      "[['in', 'that', 'is', '##', 'The', 'was', 'the', 'not', 'as', 'it', 'be', 'are', 'I', 'have', 'he', 'will', 'has', '####', 'his', 'an', 'this', 'or', 'their', 'they', 'but', '$', 'had', 'year', 'were', 'we', 'more', '###', 'up', 'been', 'you', 'its', 'one', 'would', 'which', 'out', 'can', 'It', 'all', 'also', 'two', 'after', 'first', 'He', 'do', 'time', 'than', 'when', 'We', 'over', 'last', 'other', 'her', 'into', 'In', 'our', 'there', 'A', 'she', 'could', 'just', 'years', 'some', 'three', 'million', 'them', 'what', 'But', 'so', 'no', 'like', 'if', 'only', 'percent', 'get', 'did', 'him', 'back', 'because', 'now', '#.#', 'before', 'any', 'off', 'This', 'most', 'through', 'second', 'well', 'day', 'week', 'where', 'down', 'being', 'your', 'going', 'my', 'good', 'They', \"'re\", 'should', 'many', 'way', 'those', 'four', 'during', 'such', 'may', 'very', 'how', 'since', 'take', 'including', 'then', '%', 'next', '#,###', 'much', 'still', 'go', 'think', 'even', '#.##', 'see', 'say', 'five', 'us', '1', 'these', 'If', 'And', 'me', '##,###', 'That', 'know', 'does', 'both', 'There', 'want', \"'ve\", 'got', 'third', 'another', 'need', '2', 'best', 'quarter', 'today', '##.#', 'Friday', 'month', 'billion', 'Tuesday', 'come', 'Monday', 'She', 'night', 'six', 'Thursday', '###,###', 'Wednesday', 'here', 'You', 'really', 'As', '3', 'lot', \"'m\", 'put', 'half', 'months', 'am', 'Saturday', 'too', 'better', 'days', 'came', 'past', 'took', 'expected', 'great', 'pm', 'big', 'few', 'per', 'Sunday', 'little', 'ago', 'never', '5', 'until', 'went', '##th'], ['game', 'team', 'season', 'play', 'games', 'players'], ['company', 'business', 'companies'], ['state', 'State'], ['people', 'children'], ['world', 'country'], ['without'], ['use', 'used'], ['according'], ['U.S.', 'American'], ['#-#', '##-##'], ['called'], ['case'], ['show'], ['&'], ['sales'], ['said', 'says', 'told', 'added'], ['members'], ['area'], ['points', 'point'], ['made', 'make'], ['same'], ['lead'], ['place'], ['found'], ['money'], ['man'], ['long'], ['program'], ['support'], ['former'], ['early'], ['school'], ['information'], ['local'], ['by'], ['life'], ['hit'], ['number'], ['system'], ['each', 'every'], ['start'], ['family'], ['old'], ['under'], ['home'], ['based'], ['who'], ['between'], ['end'], ['with'], ['By'], ['top'], ['work'], ['report'], ['for'], ['government'], ['part'], ['run'], ['new'], ['help'], ['about'], ['against'], [], ['public'], ['around'], ['market'], ['while'], ['on'], ['city'], ['officials'], ['from'], ['at'], ['AP'], ['own'], ['For'], ['left'], ['right'], ['set'], ['high'], ['win'], ['group'], ['services', 'service'], ['police']]\n"
     ]
    }
   ],
   "source": [
    "#Agglomerative Clustering alg\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "x2 = AgglomerativeClustering(n_clusters = None, affinity='cosine', linkage = 'single', distance_threshold=0.5).fit(x)\n",
    "x2 = x2.labels_\n",
    "\n",
    "print(x2)\n",
    "\n",
    "clusters = [[] for x in range(300)]\n",
    "for i in range(1, 300):\n",
    "    clusters[x2[i]].append(z[i])\n",
    "while clusters[-1] == []:\n",
    "    clusters.pop()\n",
    "\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'This', 'That'], ['two', 'three', 'four', 'five', 'six'], ['#,###', '##,###', '###,###'], ['##', '1', '2', '3', '5'], ['Friday', 'Tuesday', 'Monday', 'Thursday', 'Wednesday', 'Saturday', 'Sunday'], ['in', 'for', 'that', 'is', 'on', 'The', 'with', 'said', 'was', 'the', 'at', 'not', 'as', 'it', 'be', 'from', 'by', 'are', 'I', 'have', 'he', 'will', 'has', '####', 'his', 'an', 'this', 'or', 'their', 'who', 'they', 'but', '$', 'had', 'year', 'were', 'we', 'more', '###', 'up', 'been', 'you', 'its', 'one', 'about', 'would', 'which', 'out', 'can', 'all', 'also', 'after', 'first', 'He', 'do', 'time', 'than', 'when', 'We', 'over', 'last', 'new', 'other', 'her', 'people', 'into', 'In', 'our', 'there', 'A', 'she', 'could', 'just', 'years', 'some', 'U.S.', 'million', 'them', 'what', 'But', 'so', 'no', 'like', 'if', 'only', 'percent', 'get', 'did', 'him', 'game', 'back', 'because', 'now', '#.#', 'before', 'company', 'any', 'team', 'against', 'off', 'most', 'made', 'through', 'make', 'second', 'state', 'well', 'day', 'season', 'says', 'week', 'where', 'while', 'down', 'being', 'government', 'your', '#-#', 'home', 'going', 'my', 'good', 'They', \"'re\", 'should', 'many', 'way', 'those', 'during', 'such', 'may', 'very', 'how', 'since', 'work', 'take', 'including', 'high', 'then', '%', 'next', 'By', 'much', 'still', 'go', 'think', 'old', 'even', '#.##', 'world', 'see', 'say', 'business', 'told', 'under', 'us', 'these', 'If', 'right', 'And', 'me', 'between', 'play', 'help', 'market', 'know', 'end', 'AP', 'long', 'information', 'points', 'does', 'both', 'There', 'part', 'around', 'police', 'want', \"'ve\", 'based', 'For', 'got', 'third', 'school', 'left', 'another', 'country', 'need', 'best', 'win', 'quarter', 'use', 'today', '##.#', 'same', 'public', 'run', 'set', 'month', 'top', 'billion', 'come', 'She', 'city', 'place', 'night', 'each', 'here', 'You', 'group', 'really', 'found', 'As', 'used', 'lot', \"'m\", 'money', 'put', 'games', 'support', 'program', 'half', 'report', 'family', 'months', 'number', 'officials', 'am', 'former', 'own', 'man', 'too', 'better', 'days', 'came', 'lead', 'life', 'American', '##-##', 'show', 'past', 'took', 'added', 'expected', 'called', 'great', 'State', 'services', 'children', 'hit', 'area', 'system', 'every', 'pm', 'big', 'service', 'few', 'per', 'members', 'early', 'point', 'start', 'companies', 'little', '&', 'case', 'ago', 'local', 'according', 'never', 'without', 'sales', 'until', 'went', 'players', '##th']]\n"
     ]
    }
   ],
   "source": [
    "#DBSCAN Clustering Alg\n",
    "from sklearn.cluster import DBSCAN\n",
    "x2 = DBSCAN(eps=0.2, min_samples=3, metric='cosine').fit(x)\n",
    "x2 = x2.labels_\n",
    "\n",
    "clusters = [[] for x in range(300)]\n",
    "for i in range(1, 300):\n",
    "    clusters[x2[i]].append(z[i])\n",
    "\n",
    "clusters = list(filter(None, clusters))\n",
    "\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['</s>', 'our', 'on', 'no', 'company', 'By', 'we', 'us', 'your', 'my'], ['in', 'where', 'the', 'In', 'during', 'at', 'when', 'here', 'then', 'what'], ['for', 'in', 'For', 'as', 'but', 'the', 'where', 'In', 'during', 'at'], ['that', 'it', 'not', 'if', 'but', 'what', 'just', 'It', 'really', 'do'], ['is', 'was', 'are', \"'re\", \"'m\", 'now', 'had', 'were', 'been', 'came'], ['##', '5', '3', '2', '###', 'six', '1', '#,###', '##,###', '#.#'], ['The', 'This', 'That', 'A', 'It', 'which', 'But', 'And', 'If', 'that'], ['with', 'between', 'in', 'had', 'while', 'by', 'over', 'both', 'through', 'where'], ['said', 'says', 'told', 'added', 'according', 'think', 'does', 'know', 'also', 'had'], ['be', 'being', 'are', 'have', 'should', 'been', 'was', 'were', \"'re\", 'these'], ['from', 'in', 'after', 'where', 'through', 'the', 'In', 'during', 'at', 'before'], ['I', \"'m\", 'my', 'me', 'we', 'you', \"'re\", 'am', 'really', 'your'], ['he', 'He', 'him', 'his', 'she', 'I', 'She', 'They', 'But', 'but'], ['will', 'can', 'would', 'should', 'could', 'may', 'want', 'did', 'do', 'need'], ['has', 'had', 'been', 'have', \"'ve\", 'since', 'was', 'were', 'they', 'we'], ['####', 'since', '##', '1', 'year', 'last', 'been', 'has', 'after', '5'], ['an', 'another', 'this', 'the', 'A', 'was', 'one', 'next', 'every', 'that'], ['or', 'any', 'your', 'can', 'you', 'if', 'no', 'not', 'never', 'my'], ['their', 'they', 'them', 'our', 'its', 'your', 'we', 'They', 'do', 'us'], ['who', 'He', 'also', 'he', 'former', 'him', 'She', 'They', 'But', 'but'], ['$', 'million', '###,###', '#.##', '#.#', '##,###', 'billion', '#,###', '###', '##.#'], ['more', 'than', 'most', 'little', 'better', 'about', 'much', 'even', 'one', 'very'], ['up', 'down', 'out', 'off', 'around', 'back', 'into', 'on', 'over', 'where'], ['all', 'these', 'those', 'other', 'some', 'both', 'many', 'are', 'including', 'such'], ['two', 'three', 'four', 'five', 'six', 'few', '##', 'some', 'many', '5'], ['first', 'second', 'third', 'last', '##th', 'next', 'half', 'three', 'six', 'ago'], ['time', 'day', 'days', 'when', 'months', 'year', 'week', 'month', 'night', 'years'], ['We', 'They', 'we', 'You', 'If', 'I', 'But', 'they', 'And', 'our'], ['new', 'next', 'will', 'another', 'the', 'own', 'last', 'first', 'this', 'can'], ['her', 'she', 'his', 'She', 'my', 'him', 'he', 'I', 'He', 'their'], ['people', 'children', 'those', 'them', 'us', 'just', 'family', 'school', 'these', 'other'], ['there', 'There', 'no', 'here', 'going', 'we', 'But', 'And', 'It', 'If'], ['U.S.', 'American', 'world', 'country', 'billion', 'government', 'AP', 'our', 'most', 'one'], ['so', 'too', 'but', 'because', 'really', 'very', 'not', 'But', 'It', 'if'], ['like', 'really', 'think', 'just', 'do', 'want', 'so', 'I', 'very', 'know'], ['only', 'one', 'just', 'not', 'but', 'even', 'two', 'three', 'five', 'four'], ['percent', '%', '#.#', '##.#', 'million', 'billion', '#.##', '###,###', '###', '##'], ['get', 'got', 'go', 'come', 'do', 'just', 'came', 'went', 'had', \"'ve\"], ['game', 'games', 'play', 'season', 'players', 'team', 'points', 'win', 'go', 'year'], ['against', 'game', 'win', 'in', '#-#', 'case', 'games', 'play', 'season', 'players'], ['made', 'make', 'came', 'had', 'no', 'did', 'get', 'do', 'if', 'come'], ['state', 'State', 'government', 'country', 'city', 'local', 'U.S.', 'Saturday', 'former', 'officials'], ['well', 'as', 'good', 'much', 'such', 'better', 'As', 'so', 'but', 'great'], ['home', 'family', 'back', 'game', 'off', 'when', 'children', 'life', 'own', 'people'], ['way', 'how', 'going', 'it', 'really', 'so', 'what', 'if', 'want', 'do'], ['work', 'do', 'go', 'get', 'come', 'so', 'want', 'know', 'not', 'did'], ['take', 'took', 'go', 'put', 'get', 'come', 'went', 'came', 'got', 'had'], ['high', 'top', 'down', 'school', 'well', 'up', 'best', 'second', 'third', 'one'], ['still', 'now', 'but', 'even', 'just', 'only', 'right', 'is', \"'re\", 'because'], ['old', 'man', 'ago', 'year', 'last', 'who', 'him', 'he', 'police', 'his'], ['see', 'know', 'think', 'do', 'get', 'say', 'really', 'want', 'not', 'did'], ['business', 'company', 'companies', 'market', 'sales', 'services', 'its', 'government', 'quarter', 'percent'], ['under', 'put', 'on', 'into', 'without', 'the', 'come', 'go', 'take', 'get'], ['help', 'need', 'support', 'better', 'can', 'do', 'want', 'should', 'services', 'money'], ['end', 'start', 'until', 'point', 'next', 'back', 'go', 'early', 'run', 'before'], ['long', 'many', 'well', 'much', 'just', 'few', 'some', 'those', 'these', 'all'], ['information', 'services', 'report', 'money', 'For', 'any', 'service', 'companies', 'business', 'support'], ['part', 'this', 'because', 'the', 'also', 'really', 'another', 'that', 'last', 'it'], ['based', 'company', 'its', 'in', 'from', 'according', 'companies', 'business', 'market', 'sales'], ['left', 'right', 'went', 'took', 'came', 'back', 'now', 'just', 'going', 'do'], ['use', 'used', 'need', 'do', 'take', 'help', 'can', 'could', 'called', 'want'], ['today', 'Thursday', 'Wednesday', 'Monday', 'Tuesday', 'Friday', 'Saturday', 'Sunday'], ['same', 'every', 'each', 'this', 'the', 'only', 'another', 'one', 'all', 'other'], ['public', 'government', 'local', 'people', 'state', 'city', 'country', 'billion', 'companies', 'area'], ['set', 'put', 'come', 'start', 'the', 'for', 'go', 'take', 'get', 'came'], ['place', 'where', 'time', 'in', 'lead', 'the', 'when', 'here', 'then', 'what'], ['group', 'team', 'who', 'members', 'company', 'program', 'players', 'game', 'season', 'games'], ['found', 'were', 'according', 'say', 'see', 'was', 'are', 'had', 'been', 'have'], ['lot', 'some', 'little', 'really', 'much', 'great', 'many', 'few', 'those', 'these'], ['number', 'many', 'two', 'these', 'those', 'four', 'some', 'few', 'all', 'three'], ['##-##', '#-#', '##', '3', '##.#', '5', 'win', 'game', 'points', 'second'], ['show', 'see', 'say', 'know', 'so', 'program', 'think', 'do', 'get', 'not'], ['past', 'over', 'last', 'ago', 'years', 'have', 'around', 'between', 'off', 'first'], ['expected', 'will', 'could', 'next', 'would', 'going', 'can', 'should', 'may', 'last'], ['hit', 'off', 'left', 'run', 'went', 'came', 'down', 'out', 'back', 'up'], ['system', 'program', 'service', 'services', 'state', 'way', 'group', 'show', 'business', 'work'], ['pm', 'am', 'Saturday', 'Sunday', 'Friday', 'Thursday', \"'m\", 'I', \"'re\", 'is'], ['big', 'great', 'good', 'lot', 'really', 'little', 'best', 'better', 'some', 'much'], ['per', '#.##', '##.#', '#.#', '$', '###', '5', 'percent', '##', '%'], ['&', 'By', '#.##', '3', 'A', '2', 'by', 'AP', 'In', 'For']]\n"
     ]
    }
   ],
   "source": [
    "#bash list of clusters\n",
    "\n",
    "def cluster300():\n",
    "    #swap to snake_case\n",
    "    touched_points = []\n",
    "    clusters = []\n",
    "\n",
    "    for i in range (300):\n",
    "        if z[i] in touched_points:\n",
    "            continue\n",
    "        curr_cluster = make_cluster(z[i])\n",
    "        clusters.append(curr_cluster)\n",
    "        # print(\"check2\")\n",
    "        # print(curr_cluster)\n",
    "        touched_points.extend(curr_cluster)\n",
    "        # print(touched_points)\n",
    "\n",
    "    #call other function to recursively add elements \n",
    "    #curr cluster = list\n",
    "    #append to total list of clusters\n",
    "    print(clusters)\n",
    "    return clusters\n",
    "\n",
    "#z = wv\n",
    "def make_cluster(word):\n",
    "    curr_cluster=[]\n",
    "    visited = []\n",
    "    queue = []\n",
    "    queue.append(word)\n",
    "    visited.append(word)\n",
    "\n",
    "    z2 = z.copy()\n",
    "\n",
    "    while len(curr_cluster) < 10 and queue:\n",
    "        curr_word = queue.pop(0)\n",
    "        if curr_word in curr_cluster:\n",
    "            break\n",
    "\n",
    "        curr_cluster.append(curr_word)\n",
    "\n",
    "        z2.remove(curr_word)\n",
    "\n",
    "        for i in range(5):\n",
    "\n",
    "            closest_word = wv.most_similar_to_given(curr_word, z2)\n",
    "            if (closest_word in z2):\n",
    "                z2.remove(closest_word)\n",
    "\n",
    "            if not (closest_word in visited):\n",
    "                queue.append(closest_word)\n",
    "                visited.append(closest_word)\n",
    "\n",
    "        z2 = z.copy()\n",
    "\n",
    "    return curr_cluster\n",
    "\n",
    "clusters = cluster300()\n",
    "\n",
    "#ALL THE METHODS ARE BASICALLY JUST BASH EVERYTHING, THEREFORE CREATE OWN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine distances for future clusters? Unfinished\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def check_clusters(clusters, data):\n",
    "    for i in range(len(clusters)):\n",
    "        for j in range(len(clusters[i])):\n",
    "            if (1-cosine(x.data, >): #need to fix lol\n",
    "                print\n",
    "        \n",
    "x = dataAnalysis(7, data)\n",
    "check_clusters(clusters, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting algo\n",
    "\n",
    "#t-ttttttt sorting fails because two very far away points could still be the same distance from middle point :(\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def clusters_ndim (data, words):\n",
    "    dim = len(data[0])\n",
    "    print(dim)\n",
    "    #basically what I want to do is sort the words for 'closeness' to the mean vector (origin) and then \n",
    "    #first save words with data\n",
    "    data = makeKeyVec(data, words)\n",
    "    #note that distance can be defined in any way sooo\n",
    "\n",
    "    #far away things can still be the same distance from the rand\n",
    "    rand = [1]*dim\n",
    "    #need to find 'mean vector' - pca can't already do the mean vector stuff for me then :()\n",
    "    data.sort(key = lambda x: (1-cosine(x.data, rand)))\n",
    "    #need to add the words to the data, but ignore it in the vectors themselves\n",
    "    #check with the words directly above and directly below (maybe x 2? and same not in cluster method)\n",
    "    #then establish clusters\n",
    "    for i in range(dim):\n",
    "        data[i].prints()\n",
    "\n",
    "clusters_ndim(x, z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyVec:\n",
    "    #figure out if cosine distance still works on pca\n",
    "    def __init__(self, data2, word2):\n",
    "        self.data = data2\n",
    "        self.word = word2\n",
    "\n",
    "    def prints(self):\n",
    "        print(str(self.word) + \": \" + str(self.data))\n",
    "\n",
    "def makeKeyVec(data, words):\n",
    "    keyvecs = []\n",
    "    for i in range(len(data)):\n",
    "        curr_keyvec = KeyVec(data[i], words)\n",
    "        keyvecs.append(curr_keyvec)\n",
    "    return keyvecs\n",
    "\n",
    "x2 = makeKeyVec(x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataAnalysis(dim, data):\n",
    "    \n",
    "    #please fix ;-;\n",
    "    maxTransform = np.asarray([\n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [],  \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    ])\n",
    "\n",
    "    maxNum = 0\n",
    "\n",
    "    if maxNum > dim:\n",
    "        maxTransform2 = np.transpose(maxTransform)\n",
    "        maxTransform2 = maxTransform2[:dim]\n",
    "        maxTransform2 = np.transpose(maxTransform2)\n",
    "        return maxTransform2\n",
    "\n",
    "    else:\n",
    "        newDim = dim - maxNum\n",
    "\n",
    "        arr2 = data[ : , newDim : data[0].__len__() ]\n",
    "        pca = PCA(n_components = newDim)\n",
    "        pca.fit(arr2)\n",
    "        arr2 = np.asarray(pca.transform(arr2))\n",
    "\n",
    "        np.transpose(maxTransform)\n",
    "        np.transpose(arr2)\n",
    "        maxTransform = np.append(maxTransform, arr2, 1)\n",
    "        np.transpose(maxTransform)\n",
    "\n",
    "        # print(type(maxTransform))\n",
    "        # print(type(arr2))\n",
    "        # print(\"Max Transform # of vectors \" + str(maxTransform.__len__()))\n",
    "        # print(\"Max Transform Size of vectors \" + str(maxTransform[0].__len__()))\n",
    "\n",
    "        maxNum = dim\n",
    "        return maxTransform\n",
    "\n",
    "# x = np.array([[1, 2, 3, 4, 5], [2, 1, 3, 3, 1], [2, 4, 1, 2, 3], [4, 3, 2, 1, 4]])\n",
    "# print(dataAnalysis(2, x))\n",
    "# print(dataAnalysis(3, x))\n",
    "\n",
    "x = dataAnalysis(2, x)\n",
    "\n",
    "#perhaps describe how to do it with gradient function and the like to pick a random dimension\n",
    "#instead of going down one dimension at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(dim, data):\n",
    "    #data = np.array(data)\n",
    "    if dim == 3 or dim == 2:\n",
    "        fig = plt.figure(figsize=(15, 15))\n",
    "        plt.clf()\n",
    "\n",
    "        if dim == 3:\n",
    "            ax = fig.add_subplot(projection = \"3d\") #this is rectilinear, 3d, etc. projection= \"3d\"\n",
    "        elif dim == 2:\n",
    "            ax = fig.add_subplot(projection = \"rectilinear\")\n",
    "        \n",
    "        ax.set_position([0, 0, 0.95, 1])\n",
    "        plt.cla()\n",
    "           \n",
    "        if dim == 3:\n",
    "            ax.scatter(data[:, 0], data[:, 1], data[:, 2])\n",
    "            #not allowed to do it in 3 dimensions; b/c they expect a point of size two in annotation function\n",
    "            # for i in range (300):\n",
    "            #     ax.annotate(f.readline(), (data[i, 0], data[i, 1], data[i, 2]))\n",
    "            #     f.close()\n",
    "        elif dim == 2:\n",
    "            ax.scatter(data[:, 0], data[:, 1])\n",
    "            # plt.ylim(-1, 3)\n",
    "            # plt.xlim(-1.5, 2)\n",
    "            for i in range (300):\n",
    "                ax.annotate(z[i], (data[i, 0], data[i, 1]))\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(\"Too many/too few dimensions to visualize\")\n",
    "        ans = \"y\"\n",
    "        #ans = input(\"Proceed with 2-D? y/n\"\n",
    "        if ans == \"y\":\n",
    "            fig = plt.figure(figsize=(15 * dim, 15 * dim))\n",
    "            fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "            for i in range(dim):\n",
    "                for j in range(i+1, dim):\n",
    "                    #dim, dim-1, (i+1)*(j+1\n",
    "                    ax = fig.add_subplot()\n",
    "                    ax.scatter(data[:, i], data[:, j])\n",
    "\n",
    "                    #with open(\"words.txt\", \"r\") as f:\n",
    "                        #ax.set_position([0, 0, 20.95, 20])\n",
    "                    for k in range (300):\n",
    "                        ax.annotate(z[i], (data[k, i], data[k, j]))\n",
    "                        #f.close()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playable(dim, data):\n",
    "    y = dataAnalysis(dim, data)\n",
    "    #figure out AI later\n",
    "    #maybe instead of coding ai, all I have to do is check number of words with similarity value \n",
    "    #biggest similarity\n",
    "    #smallest similarity\n",
    "    #level of variance?\n",
    "    #number of words with distance d away from starting word\n",
    "    #want to preserve distance, too\n",
    "    if dim >= 2: \n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDim(data, start=1, end=300):\n",
    "    print(start)\n",
    "    print(end)\n",
    "    if start >= end:\n",
    "        return end\n",
    "    else:\n",
    "        mid = (start + end)//2\n",
    "        if not playable(mid, data):\n",
    "            return findDim(data, mid+1, end)\n",
    "        else:\n",
    "            return findDim(data, start, mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.array([[1, 2, 3, 4, 5], [2, 1, 3, 3, 1], [2, 4, 1, 2, 3], [4, 3, 2, 1, 4]])\n",
    "print(len(x))\n",
    "#dataAnalysis(150, x)\n",
    "m = findDim(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f07e8f80a78d731554c2ce7ed8433d0fa1e00c779c09dd1ced00d3c38371f3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
