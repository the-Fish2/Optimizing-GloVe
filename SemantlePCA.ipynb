{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Exists\n",
      "KeyedVectors<vector_size=300, 3000000 keys>\n"
     ]
    }
   ],
   "source": [
    "# RUN general imports\n",
    "\n",
    "import os\n",
    "\n",
    "if not {\"word2vec.model\", \"words.txt\"}.issubset(set(os.listdir())):\n",
    "\n",
    "    print(\"Beginning download\")\n",
    "\n",
    "    import gensim.downloader\n",
    "\n",
    "    print(\"imported\")\n",
    "\n",
    "    wv = gensim.downloader.load(\"word2vec-google-news-300\")\n",
    "\n",
    "    print(\"loaded\")\n",
    "\n",
    "    wv.save(\"./word2vec.model\")\n",
    "\n",
    "    print(\"saved\")\n",
    "    \n",
    "    f = open(\"words.txt\", \"x\")\n",
    "    for index, word in enumerate(wv.index_to_key):\n",
    "        if index == 300:\n",
    "            break\n",
    "        f.write(word)\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "print(\"File Exists\")\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "wv = KeyedVectors.load(\"word2vec.model\", mmap=\"r\")\n",
    "print(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN generates x\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = []\n",
    "for i in range(300):\n",
    "    x.append(wv[i])\n",
    "    # if (i == 100000):\n",
    "    #     print(\"hi\")\n",
    "    # if (i == 1000000):\n",
    "    #     print(\"hi\")\n",
    "x = np.asarray(x)\n",
    "\n",
    "# x = np.array([])\n",
    "# for i in range(300):\n",
    "#     x = np.append(x, wv[i])\n",
    "#     print(x[i][0], end = \" \")\n",
    "# print(x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN imports\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN generates array z (words)\n",
    "z = []\n",
    "with open(\"words.txt\", 'r') as f:\n",
    "    for i in range(300):\n",
    "        s = f.readline()\n",
    "        s = s[:len(s)-1]\n",
    "        z.append(s)\n",
    "#print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.keyedvectors.KeyedVectors'>\n",
      "KeyedVectors<vector_size=300, 3000000 keys>\n",
      "KeyedVectors<vector_size=300, 300 keys>\n"
     ]
    }
   ],
   "source": [
    "#convert keyed vectors to smaller length\n",
    "print(type(wv))\n",
    "print(wv)\n",
    "iterator_for_words = (key for key in z)\n",
    "wv = wv.vectors_for_all(iterator_for_words)\n",
    "print(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "</s> our we We They But And If if even [0, 71, 38, 62, 128, 84, 169, 167, 88, 155]\n",
      "\n",
      "in where when then if even so too very really [1, 117, 61, 145, 88, 155, 85, 253, 138, 230]\n",
      "\n",
      "for in where when then if even so too very [2, 1, 117, 61, 145, 88, 155, 85, 253, 138]\n",
      "\n",
      "that it just really think know do want need can [3, 15, 76, 230, 153, 177, 58, 189, 199, 50]\n",
      "\n",
      "is was had have 've we We They But And [4, 10, 35, 21, 190, 38, 62, 128, 84, 169]\n",
      "\n",
      "on off down up out back then when if even [5, 104, 119, 41, 49, 95, 145, 61, 88, 155]\n",
      "\n",
      "## 5 3 2 1 ### #,### ##,### ###,### million [6, 293, 234, 200, 165, 40, 148, 174, 225, 81]\n",
      "\n",
      "The This It That And But If They We we [7, 105, 51, 176, 169, 84, 167, 128, 62, 38]\n",
      "\n",
      "with between over past last ago after before when then [8, 171, 63, 262, 64, 289, 55, 99, 61, 145]\n",
      "\n",
      "said says told according report information services service system program [9, 115, 162, 291, 243, 181, 269, 277, 273, 241]\n",
      "\n",
      "the this another one only just really think know do [11, 28, 197, 45, 89, 76, 230, 153, 177, 58]\n",
      "\n",
      "at in where when then if even so too very [12, 1, 117, 61, 145, 88, 155, 85, 253, 138]\n",
      "\n",
      "not do want need can could would will should may [13, 58, 189, 199, 50, 75, 47, 23, 130, 137]\n",
      "\n",
      "as well good great really think know do want need [14, 112, 127, 267, 230, 153, 177, 58, 189, 199]\n",
      "\n",
      "be being have 've we We They But And If [16, 120, 21, 190, 38, 62, 128, 84, 169, 167]\n",
      "\n",
      "from in where when then if even so too very [17, 1, 117, 61, 145, 88, 155, 85, 253, 138]\n",
      "\n",
      "by being be are were was is 're 'm I [18, 120, 16, 19, 37, 10, 4, 129, 236, 20]\n",
      "\n",
      "he He She she her his him me us them [22, 57, 218, 74, 67, 26, 93, 170, 164, 82]\n",
      "\n",
      "has had have 've we We They But And If [24, 35, 21, 190, 38, 62, 128, 84, 169, 167]\n",
      "\n",
      "#### since been has had have 've we We They [25, 140, 42, 24, 35, 21, 190, 38, 62, 128]\n",
      "\n",
      "an another this the in where when then if even [27, 197, 28, 11, 1, 117, 61, 145, 88, 155]\n",
      "\n",
      "or any no there There But And If They We [29, 101, 86, 72, 185, 84, 169, 167, 128, 62]\n",
      "\n",
      "their they we We They But And If if even [30, 32, 38, 62, 128, 84, 169, 167, 88, 155]\n",
      "\n",
      "who He She she her his him me us them [31, 57, 218, 74, 67, 26, 93, 170, 164, 82]\n",
      "\n",
      "but because so too very really think know do want [33, 96, 85, 253, 138, 230, 153, 177, 58, 189]\n",
      "\n",
      "$ million billion percent % ##.# #.# #.## per ### [34, 81, 214, 90, 146, 206, 98, 156, 279, 40]\n",
      "\n",
      "year month week day days months years ago last first [36, 212, 116, 113, 255, 245, 77, 289, 64, 56]\n",
      "\n",
      "more than much little lot some many few two three [39, 60, 150, 286, 235, 78, 131, 278, 54, 80]\n",
      "\n",
      "you You We They But And If if even so [43, 228, 62, 128, 84, 169, 167, 88, 155, 85]\n",
      "\n",
      "its their they we We They But And "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nishka\\OneDrive\\Java\\PCA\\PCAGit\\PCA-semantle\\SemantlePCA.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 99>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=94'>95</a>\u001b[0m             visited\u001b[39m.\u001b[39mappend(closest_word)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=96'>97</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m curr_cluster\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=98'>99</a>\u001b[0m cluster_gen(x)\n",
      "\u001b[1;32mc:\\Users\\Nishka\\OneDrive\\Java\\PCA\\PCAGit\\PCA-semantle\\SemantlePCA.ipynb Cell 6\u001b[0m in \u001b[0;36mcluster_gen\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39min\u001b[39;00m touched_points:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=10'>11</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=12'>13</a>\u001b[0m curr_cluster \u001b[39m=\u001b[39m make_cluster(i, data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=13'>14</a>\u001b[0m clusters\u001b[39m.\u001b[39mappend(curr_cluster)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=14'>15</a>\u001b[0m \u001b[39m# print(\"check2\")\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Nishka\\OneDrive\\Java\\PCA\\PCAGit\\PCA-semantle\\SemantlePCA.ipynb Cell 6\u001b[0m in \u001b[0;36mmake_cluster\u001b[1;34m(word, data)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=85'>86</a>\u001b[0m curr_cluster\u001b[39m.\u001b[39mappend(curr_word)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=87'>88</a>\u001b[0m \u001b[39m#for i in range(5):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=88'>89</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=89'>90</a>\u001b[0m     \u001b[39m#not closed loop anymore :()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=90'>91</a>\u001b[0m closest_word \u001b[39m=\u001b[39m find_most_similar(curr_word, data, visited)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=92'>93</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (closest_word \u001b[39min\u001b[39;00m curr_cluster):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=93'>94</a>\u001b[0m     queue\u001b[39m.\u001b[39mappend(closest_word)\n",
      "\u001b[1;32mc:\\Users\\Nishka\\OneDrive\\Java\\PCA\\PCAGit\\PCA-semantle\\SemantlePCA.ipynb Cell 6\u001b[0m in \u001b[0;36mfind_most_similar\u001b[1;34m(word, total_words, curr_cluster)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=44'>45</a>\u001b[0m     skip \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=46'>47</a>\u001b[0m \u001b[39m# if word[0] == total_words[i][0]:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=47'>48</a>\u001b[0m \u001b[39m#     for j in range(2, 300):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=48'>49</a>\u001b[0m \u001b[39m#         if (word[j] == total_words[i][j]):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=49'>50</a>\u001b[0m \u001b[39m#             skip = True\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=51'>52</a>\u001b[0m curr_val \u001b[39m=\u001b[39m cosine(u \u001b[39m=\u001b[39;49m total_words[word], v \u001b[39m=\u001b[39;49m total_words[i])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=52'>53</a>\u001b[0m \u001b[39m#print(curr_val)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000027?line=54'>55</a>\u001b[0m \u001b[39mif\u001b[39;00m skip:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\spatial\\distance.py:678\u001b[0m, in \u001b[0;36mcosine\u001b[1;34m(u, v, w)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[39mCompute the Cosine distance between 1-D arrays.\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    673\u001b[0m \n\u001b[0;32m    674\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \u001b[39m# cosine distance is also referred to as 'uncentered correlation',\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[39m#   or 'reflective correlation'\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \u001b[39m# clamp the result to 0-2\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mmin\u001b[39m(correlation(u, v, w\u001b[39m=\u001b[39;49mw, centered\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m), \u001b[39m2.0\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scipy\\spatial\\distance.py:628\u001b[0m, in \u001b[0;36mcorrelation\u001b[1;34m(u, v, w, centered)\u001b[0m\n\u001b[0;32m    626\u001b[0m     v \u001b[39m=\u001b[39m v \u001b[39m-\u001b[39m vmu\n\u001b[0;32m    627\u001b[0m uv \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(u \u001b[39m*\u001b[39m v, weights\u001b[39m=\u001b[39mw)\n\u001b[1;32m--> 628\u001b[0m uu \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49maverage(np\u001b[39m.\u001b[39;49msquare(u), weights\u001b[39m=\u001b[39;49mw)\n\u001b[0;32m    629\u001b[0m vv \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage(np\u001b[39m.\u001b[39msquare(v), weights\u001b[39m=\u001b[39mw)\n\u001b[0;32m    630\u001b[0m dist \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m uv \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msqrt(uu \u001b[39m*\u001b[39m vv)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\lib\\function_base.py:518\u001b[0m, in \u001b[0;36maverage\u001b[1;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[0;32m    515\u001b[0m     keepdims_kw \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims}\n\u001b[0;32m    517\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 518\u001b[0m     avg \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39mmean(axis, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkeepdims_kw)\n\u001b[0;32m    519\u001b[0m     scl \u001b[39m=\u001b[39m avg\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype(a\u001b[39m.\u001b[39msize\u001b[39m/\u001b[39mavg\u001b[39m.\u001b[39msize)\n\u001b[0;32m    520\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\_methods.py:168\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    164\u001b[0m arr \u001b[39m=\u001b[39m asanyarray(a)\n\u001b[0;32m    166\u001b[0m is_float16_result \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m rcount \u001b[39m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[39m=\u001b[39;49mkeepdims, where\u001b[39m=\u001b[39;49mwhere)\n\u001b[0;32m    169\u001b[0m \u001b[39mif\u001b[39;00m rcount \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m where \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m umr_any(rcount \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    170\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mMean of empty slice.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mRuntimeWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\_methods.py:71\u001b[0m, in \u001b[0;36m_count_reduce_items\u001b[1;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m where \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[39m# no boolean mask given, calculate items according to axis\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 71\u001b[0m         axis \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mrange\u001b[39;49m(arr\u001b[39m.\u001b[39;49mndim))\n\u001b[0;32m     72\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(axis, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m     73\u001b[0m         axis \u001b[39m=\u001b[39m (axis,)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#RUN Bash alg\n",
    "#bash list of clusters\n",
    "\n",
    "def cluster_gen(data):\n",
    "    #swap to snake_case\n",
    "    touched_points = []\n",
    "    clusters = []\n",
    "\n",
    "    for i in range (300):\n",
    "        if i in touched_points:\n",
    "            continue\n",
    "\n",
    "        curr_cluster = make_cluster(i, data)\n",
    "        clusters.append(curr_cluster)\n",
    "        # print(\"check2\")\n",
    "        print(curr_cluster)\n",
    "        touched_points.extend(curr_cluster)\n",
    "        # print(touched_points)\n",
    "\n",
    "    #call other function to recursively add elements \n",
    "    #curr cluster = list\n",
    "    #append to total list of clusters\n",
    "    # for i in range(len(clusters)):\n",
    "    #     for j in range(len(clusters[i])):\n",
    "    #         print(z[j], end = \" \")\n",
    "    #     print()\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def find_most_similar(word, total_words, curr_cluster):\n",
    "\n",
    "    from scipy.spatial.distance import cosine\n",
    "\n",
    "    m = 0\n",
    "    while m in curr_cluster:\n",
    "        m = m + 1\n",
    "\n",
    "    min_val = cosine(u = total_words[word], v = total_words[m])\n",
    "    index = m\n",
    "\n",
    "    for i in range(m+1, 300):\n",
    "        skip = False\n",
    "\n",
    "        if i in curr_cluster:\n",
    "            skip = True\n",
    "\n",
    "        # if word[0] == total_words[i][0]:\n",
    "        #     for j in range(2, 300):\n",
    "        #         if (word[j] == total_words[i][j]):\n",
    "        #             skip = True\n",
    "        \n",
    "        curr_val = cosine(u = total_words[word], v = total_words[i])\n",
    "        #print(curr_val)\n",
    "\n",
    "        if skip:\n",
    "            curr_val = min_val\n",
    "        \n",
    "        if curr_val < min_val:\n",
    "            min_val = curr_val\n",
    "            index = i\n",
    "\n",
    "    return index\n",
    "\n",
    "# ans = find_most_similar(217, x, [217])\n",
    "# print(z[ans])\n",
    "\n",
    "#z = wv\n",
    "def make_cluster(word, data):\n",
    "\n",
    "    print()\n",
    "\n",
    "    curr_cluster=[]\n",
    "    visited = []\n",
    "    queue = []\n",
    "    queue.append(word)\n",
    "    visited.append(word)\n",
    "\n",
    "    while len(curr_cluster) < 5 and queue:\n",
    "        curr_word = queue.pop(0)\n",
    "\n",
    "        print(z[curr_word], end = \" \")\n",
    "\n",
    "        if curr_word in curr_cluster:\n",
    "            break\n",
    "\n",
    "        curr_cluster.append(curr_word)\n",
    "\n",
    "        #for i in range(5):\n",
    "\n",
    "            #not closed loop anymore :()\n",
    "        closest_word = find_most_similar(curr_word, data, visited)\n",
    "\n",
    "        if not (closest_word in curr_cluster):\n",
    "            queue.append(closest_word)\n",
    "            visited.append(closest_word)\n",
    "\n",
    "    return curr_cluster\n",
    "\n",
    "cluster_gen(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  6  6 52  9  6 55 30  6  8  9  6 29 51  1 52  9  6  6  9 24 18 23  4\n",
      " 18 61 23  6 45 37 27  6 27 52 20 18 15  9 27  7 55 43 18 17  6 52  0  4\n",
      "  6 43  4 30 39  6 39 61 64 68 51 29  7 52 63  0 45  6 39 23 12  0 40 27\n",
      " 52 30 23  4 52 25 39 28 39 46 27 52 38 52 37 52 52 52 44 11 51 23 10 43\n",
      " 52 52  3 61 49 37  5  5 43 30  1 53  0 53 64 14  1 29 15  8 15 52  6 43\n",
      "  9 14 17 36 57 52 24 22 63 34  4 39 52 39 39 61  1  4  1 52 61 11 48  6\n",
      " 58 52 44 45 59 47  7 52 11 42 16 52  3 28 42 42 49 39  8  0 24 55 39 38\n",
      " 13 38 24  0 10 65 59 49 30 42 29 67  1 41 32 51 39 56  6  0 69 51 18 26\n",
      " 40 11 64 12 13 45 28 51 55 22 62 35 33 29  3 45 14 11 50 29 15 58 46 50\n",
      " 11 50 68 14 29 50 39 45 50 59 50 52 17  5 42  2 38 33 55  7 34 20 11 10\n",
      " 65 21 35  2 57 25 39 19 31  5  6 16 50  1 22 25 11 62 57 28 36  2  0 48\n",
      "  8  4  6 22 60 54 12 13 14 21 45 31 22 54 39  3 19 50 61 32 29 49  7 26\n",
      "  5 25 14  8 18 55 37 66 61 11 10 64]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['about', 'over', 'into', 'through', 'under', 'between', 'around', 'past'],\n",
       " ['as', 'most', 'well', 'such', 'very', 'long', 'too'],\n",
       " ['found', 'report', 'show'],\n",
       " ['#.#', '#.##', '##.#', 'per'],\n",
       " ['will', 'would', 'can', 'could', 'should', 'may', 'expected'],\n",
       " ['team', 'against', 'group', 'former', 'case'],\n",
       " ['in',\n",
       "  'for',\n",
       "  'on',\n",
       "  'with',\n",
       "  'the',\n",
       "  'from',\n",
       "  'by',\n",
       "  'an',\n",
       "  'who',\n",
       "  'its',\n",
       "  'which',\n",
       "  'also',\n",
       "  'new',\n",
       "  'while',\n",
       "  'including',\n",
       "  'part',\n",
       "  'own',\n",
       "  'called'],\n",
       " ['more', 'than', 'much', 'lot', 'little'],\n",
       " ['said', 'says', 'told', 'added', 'according'],\n",
       " ['is', 'was', 'be', 'are', 'were', 'being'],\n",
       " ['game', 'play', 'games', 'players'],\n",
       " ['get', 'work', 'go', 'got', 'run', 'come', 'put', 'came', 'went'],\n",
       " ['people', 'school', 'children'],\n",
       " ['right', 'left', 'hit'],\n",
       " ['state', 'government', 'public', 'city', 'area', 'local'],\n",
       " ['year', 'season', 'week', 'month'],\n",
       " ['old', 'man'],\n",
       " ['you', 'your', 'You'],\n",
       " ['have', 'has', 'had', 'been', \"'ve\", 'never'],\n",
       " ['officials', 'members'],\n",
       " ['$', 'money'],\n",
       " ['program', 'system'],\n",
       " ['good', 'best', 'better', 'great', 'big'],\n",
       " ['he', 'his', 'her', 'she', 'him'],\n",
       " ['I', 'my', 'us', 'me'],\n",
       " ['years', 'months', 'days', 'ago'],\n",
       " ['based', '&'],\n",
       " ['their', 'they', 'we', 'our', 'them'],\n",
       " ['U.S.', 'world', 'country', 'American'],\n",
       " ['at', 'time', 'day', 'end', 'today', 'set', 'place', 'start'],\n",
       " ['The', 'It', 'A', 'This', 'That'],\n",
       " ['am', 'pm'],\n",
       " ['points', 'point'],\n",
       " ['use', 'used'],\n",
       " [\"'re\", \"'m\"],\n",
       " ['quarter', 'half'],\n",
       " ['#-#', '##-##'],\n",
       " ['or', 'no', 'any', 'without'],\n",
       " ['But', 'If', 'And', 'As'],\n",
       " ['all',\n",
       "  'two',\n",
       "  'other',\n",
       "  'some',\n",
       "  'three',\n",
       "  'many',\n",
       "  'those',\n",
       "  'four',\n",
       "  'five',\n",
       "  'these',\n",
       "  'both',\n",
       "  'six',\n",
       "  'number',\n",
       "  'few'],\n",
       " ['In', 'For'],\n",
       " ['information'],\n",
       " ['think', 'see', 'say', 'know', 'really'],\n",
       " ['up', 'out', 'back', 'off', 'down'],\n",
       " ['percent', '%'],\n",
       " ['this', 'last', 'next', 'another', 'same', 'each', 'every'],\n",
       " ['million', 'billion'],\n",
       " ['By'],\n",
       " ['take', 'took'],\n",
       " ['company', 'business', 'market', 'companies'],\n",
       " ['Friday',\n",
       "  'Tuesday',\n",
       "  'Monday',\n",
       "  'night',\n",
       "  'Thursday',\n",
       "  'Wednesday',\n",
       "  'Saturday',\n",
       "  'Sunday'],\n",
       " ['not', 'do', 'did', 'does', 'want', 'need'],\n",
       " ['that',\n",
       "  'it',\n",
       "  'but',\n",
       "  'one',\n",
       "  'when',\n",
       "  'there',\n",
       "  'just',\n",
       "  'what',\n",
       "  'so',\n",
       "  'like',\n",
       "  'if',\n",
       "  'only',\n",
       "  'because',\n",
       "  'now',\n",
       "  'where',\n",
       "  'going',\n",
       "  'way',\n",
       "  'how',\n",
       "  'then',\n",
       "  'still',\n",
       "  'even',\n",
       "  'here'],\n",
       " ['made', 'make'],\n",
       " ['services', 'service'],\n",
       " ['##', '###', '1', '2', '3', '5'],\n",
       " ['There'],\n",
       " ['home', 'family', 'life'],\n",
       " ['high', 'top'],\n",
       " ['#,###', '##,###', '###,###'],\n",
       " ['State'],\n",
       " ['####', 'after', 'before', 'during', 'since', 'early', 'until'],\n",
       " ['win', 'lead'],\n",
       " ['We', 'They'],\n",
       " ['first', 'second', 'third', '##th'],\n",
       " ['help', 'support'],\n",
       " ['sales'],\n",
       " ['AP'],\n",
       " ['He', 'She'],\n",
       " ['police']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN Birch Alg + Clusters\n",
    "def run_birch(x):\n",
    "    from sklearn.cluster import Birch\n",
    "\n",
    "    brc = Birch(n_clusters = 70)\n",
    "    x2 = brc.fit_predict(x)\n",
    "\n",
    "    print(x2)\n",
    "\n",
    "    clusters = [[] for x in range(70)]\n",
    "    for i in range(1, 300):\n",
    "        clusters[x2[i]].append(z[i])\n",
    "\n",
    "    return clusters\n",
    "\n",
    "run_birch(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN storing clusters\n",
    "clusters300 = [['about', 'over', 'into', 'through', 'under', 'between', 'around', 'past'], ['as', 'most', 'well', 'such', 'very', 'long', 'too'], ['found', 'report', 'show'], ['#.#', '#.##', '##.#', 'per'], ['will', 'would', 'can', 'could', 'should', 'may', 'expected'], ['team', 'against', 'group', 'former', 'case'], ['in', 'for', 'on', 'with', 'the', 'from', 'by', 'an', 'who', 'its', 'which', 'also', 'new', 'while', 'including', 'part', 'own', 'called'], ['more', 'than', 'much', 'lot', 'little'], ['said', 'says', 'told', 'added', 'according'], ['is', 'was', 'be', 'are', 'were', 'being'], ['game', 'play', 'games', 'players'], ['get', 'work', 'go', 'got', 'run', 'come', 'put', 'came', 'went'], ['people', 'school', 'children'], ['right', 'left', 'hit'], ['state', 'government', 'public', 'city', 'area', 'local'], ['year', 'season', 'week', 'month'], ['old', 'man'], ['you', 'your', 'You'], ['have', 'has', 'had', 'been', \"'ve\", 'never'], ['officials', 'members'], ['$', 'money'], ['program', 'system'], ['good', 'best', 'better', 'great', 'big'], ['he', 'his', 'her', 'she', 'him'], ['I', 'my', 'us', 'me'], ['years', 'months', 'days', 'ago'], ['based', '&'], ['their', 'they', 'we', 'our', 'them'], ['U.S.', 'world', 'country', 'American'], ['at', 'time', 'day', 'end', 'today', 'set', 'place', 'start'], ['The', 'It', 'A', 'This', 'That'], ['am', 'pm'], ['points', 'point'], ['use', 'used'], [\"'re\", \"'m\"], ['quarter', 'half'], ['#-#', '##-##'], ['or', 'no', 'any', 'without'], ['But', 'If', 'And', 'As'], ['all', 'two', 'other', 'some', 'three', 'many', 'those', 'four', 'five', 'these', 'both', 'six', 'number', 'few'], ['In', 'For'], ['information'], ['think', 'see', 'say', 'know', 'really'], ['up', 'out', 'back', 'off', 'down'], ['percent', '%'], ['this', 'last', 'next', 'another', 'same', 'each', 'every'], ['million', 'billion'], ['By'], ['take', 'took'], ['company', 'business', 'market', 'companies'], ['Friday', 'Tuesday', 'Monday', 'night', 'Thursday', 'Wednesday', 'Saturday', 'Sunday'], ['not', 'do', 'did', 'does', 'want', 'need'], ['that', 'it', 'but', 'one', 'when', 'there', 'just', 'what', 'so', 'like', 'if', 'only', 'because', 'now', 'where', 'going', 'way', 'how', 'then', 'still', 'even', 'here'], ['made', 'make'], ['services', 'service'], ['##', '###', '1', '2', '3', '5'], ['There'], ['home', 'family', 'life'], ['high', 'top'], ['#,###', '##,###', '###,###'], ['State'], ['####', 'after', 'before', 'during', 'since', 'early', 'until'], ['win', 'lead'], ['We', 'They'], ['first', 'second', 'third', '##th'], ['help', 'support'], ['sales'], ['AP'], ['He', 'She'], ['police']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe just like\n",
    "#add in 'new element' and if adds into another cluster with minimum distance < x\n",
    "#then allow it?\n",
    "#this may not be necessary; all that has to be done to check is like to confirm that faraway points still faraway\n",
    "#only really an issue if in closest cluster though :) - because if it's closest, then it'll ruin the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63  0 55  0  0 68  0  0 50 16  0  0 72  0  0  0  0 71 35  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0 47  0  0  0  0  0  0  0  0  0  0  0  0  0  0 61  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 59  0  0  4  0  0  0\n",
      "  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0\n",
      "  0  0  0  0  2  0  1 62  0  0  0 20  0 20  0  3  0  0  1 16  0  0 67  0\n",
      "  0 56  0 10 45  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 53  0  0\n",
      " 79  0  0  0  0 51  0  0  0  0 43  0  0  5  0  0  2  0 16 44  0  0  0  0\n",
      " 77  0  0 48  1 60  0 66  0  0 49 73 27 33 19  0  0  0 57 65 83  0  0 46\n",
      " 75  0  0 32 76  0  5  0  0  0 80  0  7  0  0 21 64 58  0 78  0 52  0  0\n",
      "  0  0  0 69 23  0  0 40  0  0  0  0  0 81  0 24  0  7  0  0  0 25  0  1\n",
      " 29 28  0 54 42  0 38 70  0 30 74 26  0  0  0  0  0 22 36  9 10 13  0  0\n",
      " 16  0 11  0  3 82  4 37 18 39 40  0  0 82  0  0 17  0 31 19 41  2  0 14\n",
      " 12  0 34  8  0  0  6 15  0  0  1  0]\n",
      "[['in', 'that', 'is', '##', 'The', 'was', 'the', 'not', 'as', 'it', 'be', 'are', 'I', 'have', 'he', 'will', 'has', '####', 'his', 'an', 'this', 'or', 'their', 'they', 'but', '$', 'had', 'year', 'were', 'we', 'more', '###', 'up', 'been', 'you', 'its', 'one', 'would', 'which', 'out', 'can', 'It', 'all', 'also', 'two', 'after', 'first', 'He', 'do', 'time', 'than', 'when', 'We', 'over', 'last', 'other', 'her', 'into', 'In', 'our', 'there', 'A', 'she', 'could', 'just', 'years', 'some', 'three', 'million', 'them', 'what', 'But', 'so', 'no', 'like', 'if', 'only', 'percent', 'get', 'did', 'him', 'back', 'because', 'now', '#.#', 'before', 'any', 'off', 'This', 'most', 'through', 'second', 'well', 'day', 'week', 'where', 'down', 'being', 'your', 'going', 'my', 'good', 'They', \"'re\", 'should', 'many', 'way', 'those', 'four', 'during', 'such', 'may', 'very', 'how', 'since', 'take', 'including', 'then', '%', 'next', '#,###', 'much', 'still', 'go', 'think', 'even', '#.##', 'see', 'say', 'five', 'us', '1', 'these', 'If', 'And', 'me', '##,###', 'That', 'know', 'does', 'both', 'There', 'want', \"'ve\", 'got', 'third', 'another', 'need', '2', 'best', 'quarter', 'today', '##.#', 'Friday', 'month', 'billion', 'Tuesday', 'come', 'Monday', 'She', 'night', 'six', 'Thursday', '###,###', 'Wednesday', 'here', 'You', 'really', 'As', '3', 'lot', \"'m\", 'put', 'half', 'months', 'am', 'Saturday', 'too', 'better', 'days', 'came', 'past', 'took', 'expected', 'great', 'pm', 'big', 'few', 'per', 'Sunday', 'little', 'ago', 'never', '5', 'until', 'went', '##th'], ['game', 'team', 'season', 'play', 'games', 'players'], ['company', 'business', 'companies'], ['state', 'State'], ['people', 'children'], ['world', 'country'], ['without'], ['use', 'used'], ['according'], ['U.S.', 'American'], ['#-#', '##-##'], ['called'], ['case'], ['show'], ['&'], ['sales'], ['said', 'says', 'told', 'added'], ['members'], ['area'], ['points', 'point'], ['made', 'make'], ['same'], ['lead'], ['place'], ['found'], ['money'], ['man'], ['long'], ['program'], ['support'], ['former'], ['early'], ['school'], ['information'], ['local'], ['by'], ['life'], ['hit'], ['number'], ['system'], ['each', 'every'], ['start'], ['family'], ['old'], ['under'], ['home'], ['based'], ['who'], ['between'], ['end'], ['with'], ['By'], ['top'], ['work'], ['report'], ['for'], ['government'], ['part'], ['run'], ['new'], ['help'], ['about'], ['against'], [], ['public'], ['around'], ['market'], ['while'], ['on'], ['city'], ['officials'], ['from'], ['at'], ['AP'], ['own'], ['For'], ['left'], ['right'], ['set'], ['high'], ['win'], ['group'], ['services', 'service'], ['police']]\n"
     ]
    }
   ],
   "source": [
    "#Agglomerative Clustering alg\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "x2 = AgglomerativeClustering(n_clusters = None, affinity='cosine', linkage = 'single', distance_threshold=0.5).fit(x)\n",
    "x2 = x2.labels_\n",
    "\n",
    "print(x2)\n",
    "\n",
    "clusters = [[] for x in range(300)]\n",
    "for i in range(1, 300):\n",
    "    clusters[x2[i]].append(z[i])\n",
    "while clusters[-1] == []:\n",
    "    clusters.pop()\n",
    "\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['It', 'This', 'That'], ['two', 'three', 'four', 'five', 'six'], ['#,###', '##,###', '###,###'], ['##', '1', '2', '3', '5'], ['Friday', 'Tuesday', 'Monday', 'Thursday', 'Wednesday', 'Saturday', 'Sunday'], ['in', 'for', 'that', 'is', 'on', 'The', 'with', 'said', 'was', 'the', 'at', 'not', 'as', 'it', 'be', 'from', 'by', 'are', 'I', 'have', 'he', 'will', 'has', '####', 'his', 'an', 'this', 'or', 'their', 'who', 'they', 'but', '$', 'had', 'year', 'were', 'we', 'more', '###', 'up', 'been', 'you', 'its', 'one', 'about', 'would', 'which', 'out', 'can', 'all', 'also', 'after', 'first', 'He', 'do', 'time', 'than', 'when', 'We', 'over', 'last', 'new', 'other', 'her', 'people', 'into', 'In', 'our', 'there', 'A', 'she', 'could', 'just', 'years', 'some', 'U.S.', 'million', 'them', 'what', 'But', 'so', 'no', 'like', 'if', 'only', 'percent', 'get', 'did', 'him', 'game', 'back', 'because', 'now', '#.#', 'before', 'company', 'any', 'team', 'against', 'off', 'most', 'made', 'through', 'make', 'second', 'state', 'well', 'day', 'season', 'says', 'week', 'where', 'while', 'down', 'being', 'government', 'your', '#-#', 'home', 'going', 'my', 'good', 'They', \"'re\", 'should', 'many', 'way', 'those', 'during', 'such', 'may', 'very', 'how', 'since', 'work', 'take', 'including', 'high', 'then', '%', 'next', 'By', 'much', 'still', 'go', 'think', 'old', 'even', '#.##', 'world', 'see', 'say', 'business', 'told', 'under', 'us', 'these', 'If', 'right', 'And', 'me', 'between', 'play', 'help', 'market', 'know', 'end', 'AP', 'long', 'information', 'points', 'does', 'both', 'There', 'part', 'around', 'police', 'want', \"'ve\", 'based', 'For', 'got', 'third', 'school', 'left', 'another', 'country', 'need', 'best', 'win', 'quarter', 'use', 'today', '##.#', 'same', 'public', 'run', 'set', 'month', 'top', 'billion', 'come', 'She', 'city', 'place', 'night', 'each', 'here', 'You', 'group', 'really', 'found', 'As', 'used', 'lot', \"'m\", 'money', 'put', 'games', 'support', 'program', 'half', 'report', 'family', 'months', 'number', 'officials', 'am', 'former', 'own', 'man', 'too', 'better', 'days', 'came', 'lead', 'life', 'American', '##-##', 'show', 'past', 'took', 'added', 'expected', 'called', 'great', 'State', 'services', 'children', 'hit', 'area', 'system', 'every', 'pm', 'big', 'service', 'few', 'per', 'members', 'early', 'point', 'start', 'companies', 'little', '&', 'case', 'ago', 'local', 'according', 'never', 'without', 'sales', 'until', 'went', 'players', '##th']]\n"
     ]
    }
   ],
   "source": [
    "#DBSCAN Clustering Alg\n",
    "from sklearn.cluster import DBSCAN\n",
    "x2 = DBSCAN(eps=0.2, min_samples=3, metric='cosine').fit(x)\n",
    "x2 = x2.labels_\n",
    "\n",
    "clusters = [[] for x in range(300)]\n",
    "for i in range(1, 300):\n",
    "    clusters[x2[i]].append(z[i])\n",
    "\n",
    "clusters = list(filter(None, clusters))\n",
    "\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['</s>', 'our', 'on', 'no', 'company', 'By', 'we', 'us', 'your', 'my'], ['in', 'where', 'the', 'In', 'during', 'at', 'when', 'here', 'then', 'what'], ['for', 'in', 'For', 'as', 'but', 'the', 'where', 'In', 'during', 'at'], ['that', 'it', 'not', 'if', 'but', 'what', 'just', 'It', 'really', 'do'], ['is', 'was', 'are', \"'re\", \"'m\", 'now', 'had', 'were', 'been', 'came'], ['##', '5', '3', '2', '###', 'six', '1', '#,###', '##,###', '#.#'], ['The', 'This', 'That', 'A', 'It', 'which', 'But', 'And', 'If', 'that'], ['with', 'between', 'in', 'had', 'while', 'by', 'over', 'both', 'through', 'where'], ['said', 'says', 'told', 'added', 'according', 'think', 'does', 'know', 'also', 'had'], ['be', 'being', 'are', 'have', 'should', 'been', 'was', 'were', \"'re\", 'these'], ['from', 'in', 'after', 'where', 'through', 'the', 'In', 'during', 'at', 'before'], ['I', \"'m\", 'my', 'me', 'we', 'you', \"'re\", 'am', 'really', 'your'], ['he', 'He', 'him', 'his', 'she', 'I', 'She', 'They', 'But', 'but'], ['will', 'can', 'would', 'should', 'could', 'may', 'want', 'did', 'do', 'need'], ['has', 'had', 'been', 'have', \"'ve\", 'since', 'was', 'were', 'they', 'we'], ['####', 'since', '##', '1', 'year', 'last', 'been', 'has', 'after', '5'], ['an', 'another', 'this', 'the', 'A', 'was', 'one', 'next', 'every', 'that'], ['or', 'any', 'your', 'can', 'you', 'if', 'no', 'not', 'never', 'my'], ['their', 'they', 'them', 'our', 'its', 'your', 'we', 'They', 'do', 'us'], ['who', 'He', 'also', 'he', 'former', 'him', 'She', 'They', 'But', 'but'], ['$', 'million', '###,###', '#.##', '#.#', '##,###', 'billion', '#,###', '###', '##.#'], ['more', 'than', 'most', 'little', 'better', 'about', 'much', 'even', 'one', 'very'], ['up', 'down', 'out', 'off', 'around', 'back', 'into', 'on', 'over', 'where'], ['all', 'these', 'those', 'other', 'some', 'both', 'many', 'are', 'including', 'such'], ['two', 'three', 'four', 'five', 'six', 'few', '##', 'some', 'many', '5'], ['first', 'second', 'third', 'last', '##th', 'next', 'half', 'three', 'six', 'ago'], ['time', 'day', 'days', 'when', 'months', 'year', 'week', 'month', 'night', 'years'], ['We', 'They', 'we', 'You', 'If', 'I', 'But', 'they', 'And', 'our'], ['new', 'next', 'will', 'another', 'the', 'own', 'last', 'first', 'this', 'can'], ['her', 'she', 'his', 'She', 'my', 'him', 'he', 'I', 'He', 'their'], ['people', 'children', 'those', 'them', 'us', 'just', 'family', 'school', 'these', 'other'], ['there', 'There', 'no', 'here', 'going', 'we', 'But', 'And', 'It', 'If'], ['U.S.', 'American', 'world', 'country', 'billion', 'government', 'AP', 'our', 'most', 'one'], ['so', 'too', 'but', 'because', 'really', 'very', 'not', 'But', 'It', 'if'], ['like', 'really', 'think', 'just', 'do', 'want', 'so', 'I', 'very', 'know'], ['only', 'one', 'just', 'not', 'but', 'even', 'two', 'three', 'five', 'four'], ['percent', '%', '#.#', '##.#', 'million', 'billion', '#.##', '###,###', '###', '##'], ['get', 'got', 'go', 'come', 'do', 'just', 'came', 'went', 'had', \"'ve\"], ['game', 'games', 'play', 'season', 'players', 'team', 'points', 'win', 'go', 'year'], ['against', 'game', 'win', 'in', '#-#', 'case', 'games', 'play', 'season', 'players'], ['made', 'make', 'came', 'had', 'no', 'did', 'get', 'do', 'if', 'come'], ['state', 'State', 'government', 'country', 'city', 'local', 'U.S.', 'Saturday', 'former', 'officials'], ['well', 'as', 'good', 'much', 'such', 'better', 'As', 'so', 'but', 'great'], ['home', 'family', 'back', 'game', 'off', 'when', 'children', 'life', 'own', 'people'], ['way', 'how', 'going', 'it', 'really', 'so', 'what', 'if', 'want', 'do'], ['work', 'do', 'go', 'get', 'come', 'so', 'want', 'know', 'not', 'did'], ['take', 'took', 'go', 'put', 'get', 'come', 'went', 'came', 'got', 'had'], ['high', 'top', 'down', 'school', 'well', 'up', 'best', 'second', 'third', 'one'], ['still', 'now', 'but', 'even', 'just', 'only', 'right', 'is', \"'re\", 'because'], ['old', 'man', 'ago', 'year', 'last', 'who', 'him', 'he', 'police', 'his'], ['see', 'know', 'think', 'do', 'get', 'say', 'really', 'want', 'not', 'did'], ['business', 'company', 'companies', 'market', 'sales', 'services', 'its', 'government', 'quarter', 'percent'], ['under', 'put', 'on', 'into', 'without', 'the', 'come', 'go', 'take', 'get'], ['help', 'need', 'support', 'better', 'can', 'do', 'want', 'should', 'services', 'money'], ['end', 'start', 'until', 'point', 'next', 'back', 'go', 'early', 'run', 'before'], ['long', 'many', 'well', 'much', 'just', 'few', 'some', 'those', 'these', 'all'], ['information', 'services', 'report', 'money', 'For', 'any', 'service', 'companies', 'business', 'support'], ['part', 'this', 'because', 'the', 'also', 'really', 'another', 'that', 'last', 'it'], ['based', 'company', 'its', 'in', 'from', 'according', 'companies', 'business', 'market', 'sales'], ['left', 'right', 'went', 'took', 'came', 'back', 'now', 'just', 'going', 'do'], ['use', 'used', 'need', 'do', 'take', 'help', 'can', 'could', 'called', 'want'], ['today', 'Thursday', 'Wednesday', 'Monday', 'Tuesday', 'Friday', 'Saturday', 'Sunday'], ['same', 'every', 'each', 'this', 'the', 'only', 'another', 'one', 'all', 'other'], ['public', 'government', 'local', 'people', 'state', 'city', 'country', 'billion', 'companies', 'area'], ['set', 'put', 'come', 'start', 'the', 'for', 'go', 'take', 'get', 'came'], ['place', 'where', 'time', 'in', 'lead', 'the', 'when', 'here', 'then', 'what'], ['group', 'team', 'who', 'members', 'company', 'program', 'players', 'game', 'season', 'games'], ['found', 'were', 'according', 'say', 'see', 'was', 'are', 'had', 'been', 'have'], ['lot', 'some', 'little', 'really', 'much', 'great', 'many', 'few', 'those', 'these'], ['number', 'many', 'two', 'these', 'those', 'four', 'some', 'few', 'all', 'three'], ['##-##', '#-#', '##', '3', '##.#', '5', 'win', 'game', 'points', 'second'], ['show', 'see', 'say', 'know', 'so', 'program', 'think', 'do', 'get', 'not'], ['past', 'over', 'last', 'ago', 'years', 'have', 'around', 'between', 'off', 'first'], ['expected', 'will', 'could', 'next', 'would', 'going', 'can', 'should', 'may', 'last'], ['hit', 'off', 'left', 'run', 'went', 'came', 'down', 'out', 'back', 'up'], ['system', 'program', 'service', 'services', 'state', 'way', 'group', 'show', 'business', 'work'], ['pm', 'am', 'Saturday', 'Sunday', 'Friday', 'Thursday', \"'m\", 'I', \"'re\", 'is'], ['big', 'great', 'good', 'lot', 'really', 'little', 'best', 'better', 'some', 'much'], ['per', '#.##', '##.#', '#.#', '$', '###', '5', 'percent', '##', '%'], ['&', 'By', '#.##', '3', 'A', '2', 'by', 'AP', 'In', 'For']]\n"
     ]
    }
   ],
   "source": [
    "#bash list of clusters\n",
    "\n",
    "def cluster300():\n",
    "    #swap to snake_case\n",
    "    touched_points = []\n",
    "    clusters = []\n",
    "\n",
    "    for i in range (300):\n",
    "        if z[i] in touched_points:\n",
    "            continue\n",
    "        curr_cluster = make_cluster(z[i])\n",
    "        clusters.append(curr_cluster)\n",
    "        # print(\"check2\")\n",
    "        # print(curr_cluster)\n",
    "        touched_points.extend(curr_cluster)\n",
    "        # print(touched_points)\n",
    "\n",
    "    #call other function to recursively add elements \n",
    "    #curr cluster = list\n",
    "    #append to total list of clusters\n",
    "    print(clusters)\n",
    "    return clusters\n",
    "\n",
    "#z = wv\n",
    "def make_cluster(word):\n",
    "    curr_cluster=[]\n",
    "    visited = []\n",
    "    queue = []\n",
    "    queue.append(word)\n",
    "    visited.append(word)\n",
    "\n",
    "    z2 = z.copy()\n",
    "\n",
    "    while len(curr_cluster) < 10 and queue:\n",
    "        curr_word = queue.pop(0)\n",
    "        if curr_word in curr_cluster:\n",
    "            break\n",
    "\n",
    "        curr_cluster.append(curr_word)\n",
    "\n",
    "        z2.remove(curr_word)\n",
    "\n",
    "        for i in range(5):\n",
    "\n",
    "            closest_word = wv.most_similar_to_given(curr_word, z2)\n",
    "            if (closest_word in z2):\n",
    "                z2.remove(closest_word)\n",
    "\n",
    "            if not (closest_word in visited):\n",
    "                queue.append(closest_word)\n",
    "                visited.append(closest_word)\n",
    "\n",
    "        z2 = z.copy()\n",
    "\n",
    "    return curr_cluster\n",
    "\n",
    "clusters = cluster300()\n",
    "\n",
    "#ALL THE METHODS ARE BASICALLY JUST BASH EVERYTHING, THEREFORE CREATE OWN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine distances for future clusters? Unfinished\n",
    "\n",
    "\n",
    "\n",
    "def check_clusters(clusters, data):\n",
    "    for i in range(len(clusters)):\n",
    "        for j in range(len(clusters[i])):\n",
    "            if (1-cosine(x.data, >): #need to fix lol\n",
    "                print\n",
    "        \n",
    "x = dataAnalysis(7, data)\n",
    "check_clusters(clusters, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting algo\n",
    "\n",
    "#t-ttttttt sorting fails because two very far away points could still be the same distance from middle point :(\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def clusters_ndim (data, words):\n",
    "    dim = len(data[0])\n",
    "    print(dim)\n",
    "    #basically what I want to do is sort the words for 'closeness' to the mean vector (origin) and then \n",
    "    #first save words with data\n",
    "    data = makeKeyVec(data, words)\n",
    "    #note that distance can be defined in any way sooo\n",
    "\n",
    "    #far away things can still be the same distance from the rand\n",
    "    rand = [1]*dim\n",
    "    #need to find 'mean vector' - pca can't already do the mean vector stuff for me then :()\n",
    "    data.sort(key = lambda x: (1-cosine(x.data, rand)))\n",
    "    #need to add the words to the data, but ignore it in the vectors themselves\n",
    "    #check with the words directly above and directly below (maybe x 2? and same not in cluster method)\n",
    "    #then establish clusters\n",
    "    for i in range(dim):\n",
    "        data[i].prints()\n",
    "\n",
    "clusters_ndim(x, z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nishka\\OneDrive\\Java\\PCA\\PCAGit\\PCA-semantle\\SemantlePCA.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000020?line=13'>14</a>\u001b[0m         keyvecs\u001b[39m.\u001b[39mappend(curr_keyvec)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000020?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m keyvecs\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nishka/OneDrive/Java/PCA/PCAGit/PCA-semantle/SemantlePCA.ipynb#ch0000020?line=16'>17</a>\u001b[0m x2 \u001b[39m=\u001b[39m makeKeyVec(x, z)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "class KeyVec:\n",
    "    #figure out if cosine distance still works on pca\n",
    "    def __init__(self, data2, word2):\n",
    "        self.data = data2\n",
    "        self.word = word2\n",
    "\n",
    "    def prints(self):\n",
    "        print(str(self.word) + \": \" + str(self.data))\n",
    "\n",
    "def makeKeyVec(data, words):\n",
    "    keyvecs = []\n",
    "    for i in range(len(data)):\n",
    "        curr_keyvec = KeyVec(data[i], words)\n",
    "        keyvecs.append(curr_keyvec)\n",
    "    return keyvecs\n",
    "\n",
    "x2 = makeKeyVec(x, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataAnalysis(dim, data):\n",
    "    \n",
    "    #please fix ;-;\n",
    "    maxTransform = np.asarray([\n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [],  \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], \n",
    "    ])\n",
    "\n",
    "    maxNum = 0\n",
    "\n",
    "    if maxNum > dim:\n",
    "        maxTransform2 = np.transpose(maxTransform)\n",
    "        maxTransform2 = maxTransform2[:dim]\n",
    "        maxTransform2 = np.transpose(maxTransform2)\n",
    "        return maxTransform2\n",
    "\n",
    "    else:\n",
    "        newDim = dim - maxNum\n",
    "\n",
    "        arr2 = data[ : , newDim : data[0].__len__() ]\n",
    "        pca = PCA(n_components = newDim)\n",
    "        pca.fit(arr2)\n",
    "        arr2 = np.asarray(pca.transform(arr2))\n",
    "\n",
    "        np.transpose(maxTransform)\n",
    "        np.transpose(arr2)\n",
    "        maxTransform = np.append(maxTransform, arr2, 1)\n",
    "        np.transpose(maxTransform)\n",
    "\n",
    "        # print(type(maxTransform))\n",
    "        # print(type(arr2))\n",
    "        # print(\"Max Transform # of vectors \" + str(maxTransform.__len__()))\n",
    "        # print(\"Max Transform Size of vectors \" + str(maxTransform[0].__len__()))\n",
    "\n",
    "        maxNum = dim\n",
    "        return maxTransform\n",
    "\n",
    "# x = np.array([[1, 2, 3, 4, 5], [2, 1, 3, 3, 1], [2, 4, 1, 2, 3], [4, 3, 2, 1, 4]])\n",
    "# print(dataAnalysis(2, x))\n",
    "# # print(dataAnalysis(3, x))\n",
    "\n",
    "# x = dataAnalysis(2, x)\n",
    "\n",
    "#perhaps describe how to do it with gradient function and the like to pick a random dimension\n",
    "#instead of going down one dimension at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(dim, data):\n",
    "    #data = np.array(data)\n",
    "    if dim == 3 or dim == 2:\n",
    "        fig = plt.figure(figsize=(15, 15))\n",
    "        plt.clf()\n",
    "\n",
    "        if dim == 3:\n",
    "            ax = fig.add_subplot(projection = \"3d\") #this is rectilinear, 3d, etc. projection= \"3d\"\n",
    "        elif dim == 2:\n",
    "            ax = fig.add_subplot(projection = \"rectilinear\")\n",
    "        \n",
    "        ax.set_position([0, 0, 0.95, 1])\n",
    "        plt.cla()\n",
    "           \n",
    "        if dim == 3:\n",
    "            ax.scatter(data[:, 0], data[:, 1], data[:, 2])\n",
    "            #not allowed to do it in 3 dimensions; b/c they expect a point of size two in annotation function\n",
    "            # for i in range (300):\n",
    "            #     ax.annotate(f.readline(), (data[i, 0], data[i, 1], data[i, 2]))\n",
    "            #     f.close()\n",
    "        elif dim == 2:\n",
    "            ax.scatter(data[:, 0], data[:, 1])\n",
    "            # plt.ylim(-1, 3)\n",
    "            # plt.xlim(-1.5, 2)\n",
    "            for i in range (300):\n",
    "                ax.annotate(z[i], (data[i, 0], data[i, 1]))\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(\"Too many/too few dimensions to visualize\")\n",
    "        ans = \"y\"\n",
    "        #ans = input(\"Proceed with 2-D? y/n\"\n",
    "        if ans == \"y\":\n",
    "            fig = plt.figure(figsize=(15 * dim, 15 * dim))\n",
    "            fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "            for i in range(dim):\n",
    "                for j in range(i+1, dim):\n",
    "                    #dim, dim-1, (i+1)*(j+1\n",
    "                    ax = fig.add_subplot()\n",
    "                    ax.scatter(data[:, i], data[:, j])\n",
    "\n",
    "                    #with open(\"words.txt\", \"r\") as f:\n",
    "                        #ax.set_position([0, 0, 20.95, 20])\n",
    "                    for k in range (300):\n",
    "                        ax.annotate(z[i], (data[k, i], data[k, j]))\n",
    "                        #f.close()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  3  0  0  0  6 12  1  0  0  9  0  9  2  0  0  0  3  3  0  2  0  0  0\n",
      "  7  3  0  3  0  0  0  0  2  0  4  6  3  0  2  0  4  6  0  2  7  0  0  0\n",
      "  3  0  2  5  0  7  3  9  6  1  2  0  3  6  5  3  6  7  0  0  0  0  1  2\n",
      "  2  1  0  0  2  7  0  7 12  4  2  2  5  2  0  2  2  0  4  2  2  0 11  6\n",
      "  0  0  4  6  7  0  6  6  6  5  0  0  0  2  8  7  0  6 11  0  9  0  3  6\n",
      "  0  7  2  8  6  2  2  2  5  2  0  0  2  0 12  9  0  0  2  2  9  0  6  3\n",
      "  3  6  4  6  4 10  2  0  2  2  0  0  4  0  2  0  7 12  0  0  2 12  2  5\n",
      "  2  5  2  3 11  0  4  7  5  2  6  9  0  7  8  2  0  5  0  0  7  2  2  3\n",
      "  1  2  8  0  6  0  7  2 12  2 11 12  0  9  4  0  7  6  9  6  9  6  4  9\n",
      "  2  9  1  7  6  6 12  3  9  4  9  2  5  7  2  0  1  0 12  2  2  0  6 11\n",
      "  7  7 12  7  7  3  3  7  0  9  0  9  9  2  2  9  6  6  0  7  8  0  0  6\n",
      "  0  3  0  2  9  7  7  6  7  7  0  9  2  7  0  4  7  9  6  6  6  7  2  3\n",
      "  0  9  7  3  2 12  0  3  6  6  6  8]\n",
      "[['for', 'that', 'is', 'with', 'said', 'the', 'as', 'it', 'be', 'are', 'have', 'he', 'will', 'his', 'this', 'or', 'their', 'who', 'but', 'were', 'more', 'been', 'one', 'about', 'would', 'out', 'all', 'time', 'other', 'her', 'people', 'into', 'she', 'could', 'some', 'no', 'only', 'him', 'because', 'now', 'any', 'most', 'made', 'through', 'well', 'says', 'where', 'being', 'should', 'many', 'those', 'such', 'may', 'work', 'still', 'old', 'even', 'world', 'say', 'told', 'under', 'help', 'long', 'both', 'part', 'around', 'school', 'another', 'use', 'same', 'found', 'used', 'money', 'am', 'own', 'life', 'show', 'past', 'added', 'called', 'every', 'few', 'case', 'without'], ['The', 'He', 'In', 'A', 'For', 'She', 'As'], ['not', 'I', 'they', 'we', 'you', 'can', 'do', 'our', 'there', 'just', 'them', 'what', 'so', 'like', 'if', 'get', 'did', 'make', 'your', 'going', 'my', 'good', \"'re\", 'way', 'very', 'how', 'much', 'go', 'think', 'see', 'us', 'these', 'right', 'me', 'know', 'does', 'want', \"'ve\", 'got', 'need', 'best', 'come', 'here', 'really', 'lot', \"'m\", 'too', 'better', 'great', 'big', 'little', 'never'], ['in', 'from', 'by', '####', 'an', 'year', 'which', 'two', 'than', 'over', 'while', 'including', 'high', 'between', 'based', 'each', 'months', 'number', 'expected', '&', 'according', 'sales'], ['$', '###', 'million', 'percent', '#.#', '%', '#,###', '#.##', '##,###', '##.#', 'billion', '###,###', 'per'], ['It', 'We', 'But', 'This', 'They', 'If', 'And', 'That', 'There', 'You'], ['on', 'had', 'up', 'first', 'when', 'last', 'back', 'before', 'team', 'against', 'off', 'day', 'down', 'home', 'take', 'then', 'next', 'end', 'left', 'run', 'set', 'top', 'place', 'night', 'put', 'came', 'lead', 'took', 'hit', 'early', 'point', 'start', 'until', 'went', 'players'], ['has', 'its', 'also', 'new', 'years', 'U.S.', 'company', 'state', 'government', 'business', 'market', 'information', 'police', 'country', 'public', 'city', 'group', 'support', 'program', 'report', 'family', 'officials', 'American', 'services', 'children', 'area', 'system', 'service', 'members', 'companies', 'local'], ['second', '#-#', 'points', 'third', '##-##', '##th'], ['was', 'at', 'after', 'week', 'during', 'since', 'AP', 'today', 'Friday', 'month', 'Tuesday', 'Monday', 'Thursday', 'Wednesday', 'former', 'man', 'Saturday', 'days', 'State', 'pm', 'Sunday', 'ago'], ['By'], ['game', 'season', 'play', 'win', 'games'], ['##', 'three', 'four', 'five', '1', '2', 'quarter', 'six', '3', 'half', '5'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishka\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\cluster\\_birch.py:747: ConvergenceWarning: Number of subclusters found (13) by BIRCH is less than (70). Decrease the threshold.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def playable(dim, data):\n",
    "    global clusters300\n",
    "\n",
    "    y = dataAnalysis(dim, data)\n",
    "    y2 = run_birch(y)\n",
    "\n",
    "    print(y2)\n",
    "\n",
    "    if dim >= 2: \n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "playable(8, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDim(data, start=1, end=300):\n",
    "    print(start)\n",
    "    print(end)\n",
    "    if start >= end:\n",
    "        return end\n",
    "    else:\n",
    "        mid = (start + end)//2\n",
    "        if not playable(mid, data):\n",
    "            return findDim(data, mid+1, end)\n",
    "        else:\n",
    "            return findDim(data, start, mid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.array([[1, 2, 3, 4, 5], [2, 1, 3, 3, 1], [2, 4, 1, 2, 3], [4, 3, 2, 1, 4]])\n",
    "#OF COURSE IT WON'T WORK - IT'S A MAX OF 4 DIMENSIONS AND YOU'RE RUNNING 150 ON IT!\n",
    "print(len(x))\n",
    "#dataAnalysis(150, x)\n",
    "m = findDim(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f07e8f80a78d731554c2ce7ed8433d0fa1e00c779c09dd1ced00d3c38371f3b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
