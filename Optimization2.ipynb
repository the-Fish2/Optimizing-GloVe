{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized BIRCH\n",
    "# from itertools import cycle\n",
    "# from time import time\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.colors as colors\n",
    "\n",
    "from sklearn.cluster import Birch\n",
    "# from sklearn.datasets import make_blobs\n",
    "# from sklearn.utils import check_array\n",
    "# from sklearn.utils.extmath import safe_sparse_dot\n",
    "\n",
    "# from memory_profiler import profile\n",
    "\n",
    "# n_decompositon = 1000  # divide the array 'reduced_distance' into 1000 parts along the axis=0\n",
    "\n",
    "\n",
    "# class OldBirch(Birch):\n",
    "#     @profile\n",
    "#     def predict(self, X):\n",
    "#         # the original code\n",
    "#         X = check_array(X, accept_sparse='csr')\n",
    "#         self._check_fit(X)\n",
    "#         reduced_distance = safe_sparse_dot(X, self.subcluster_centers_.T)\n",
    "#         reduced_distance *= -2\n",
    "#         reduced_distance += self._subcluster_norms\n",
    "#         return self.subcluster_labels_[np.argmin(reduced_distance, axis=1)]\n",
    "\n",
    "\n",
    "# class NewBirch(Birch):\n",
    "#     @profile\n",
    "#     def predict(self, X):\n",
    "#         X = check_array(X, accept_sparse='csr')\n",
    "#         self._check_fit(X)\n",
    "#         '''\n",
    "#         try:\n",
    "#             reduced_distance = safe_sparse_dot(X, self.subcluster_centers_.T)  # the original code\n",
    "#             reduced_distance *= -2\n",
    "#             reduced_distance += self._subcluster_norms\n",
    "#             return self.subcluster_labels_[np.argmin(reduced_distance, axis=1)]\n",
    "#         except MemoryError:\n",
    "#         '''\n",
    "#         # assume that the matrix is dense\n",
    "#         argmin_list = np.array([], dtype=np.int)\n",
    "#         interval = int(np.ceil(X.shape[0] / n_decompositon))\n",
    "#         for index in range(0, n_decompositon - 1):\n",
    "#             lb = index * interval\n",
    "#             ub = (index + 1) * interval\n",
    "#             reduced_distance = safe_sparse_dot(X[lb:ub, :], self.subcluster_centers_.T)\n",
    "#             reduced_distance *= -2\n",
    "#             reduced_distance += self._subcluster_norms\n",
    "#             argmin_list = np.append(argmin_list, np.argmin(reduced_distance, axis=1))\n",
    "\n",
    "#         lb = (n_decompositon - 1) * interval\n",
    "#         reduced_distance = safe_sparse_dot(X[lb:X.shape[0], :], self.subcluster_centers_.T)\n",
    "#         reduced_distance *= -2\n",
    "#         reduced_distance += self._subcluster_norms\n",
    "#         argmin_list = np.append(argmin_list, np.argmin(reduced_distance, axis=1))\n",
    "\n",
    "#         return self.subcluster_labels_[argmin_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #setting up x initially\n",
    "# x = []\n",
    "# with open(\"glove.6B.300d.txt\", 'r') as f:\n",
    "#     with open(\"WordFiles2000/gloveWords2.txt\", 'w') as f2:\n",
    "#         for i in range(20000):\n",
    "#             try:\n",
    "#                 s = next(f).split(\" \")\n",
    "#                 f2.write(s[0] + \"\\n\")\n",
    "#                 s = s[1:]\n",
    "#                 s[len(s) - 1] = s[len(s) - 1].split('\\n')[0]\n",
    "#                 x.append(s)    \n",
    "#             except(UnicodeDecodeError):\n",
    "#                 pass\n",
    "# #never run again. now use storeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #setting up x initially\n",
    "\n",
    "# keepX = x\n",
    "# print(type(keepX))\n",
    "# print(keepX[1][1])\n",
    "# print([keepX[i+1] for i in range(1)])\n",
    "# print([keepX[1][i] for i in range(len(keepX[1]))])\n",
    "# with open(\"WordFiles2000/storeX.txt\", 'w') as f:\n",
    "#     for j in range(19999):\n",
    "#         f.write(keepX[j])\n",
    "#         f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = []\n",
    "with open(\"WordFiles/storeX.txt\", 'r') as f:\n",
    "    #99969\n",
    "    for i in range(10000):  \n",
    "        s = next(f).split(\" \")\n",
    "        s = s[:len(s) - 1]\n",
    "        s = [float(i) for i in s]\n",
    "        if (len(s) == 300):\n",
    "            new_x.append(s)\n",
    "        #print(len(new_x[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(new_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = []\n",
    "with open(\"WordFiles/gloveWords.txt\", 'r') as f:\n",
    "    for i in range(10000):\n",
    "        s = f.readline()\n",
    "        s = s[:len(s)-1]\n",
    "        z.append(s)\n",
    "\n",
    "print(z[19990:19999])\n",
    "print(z[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(9999):\n",
    "    if (len(new_x[j]) != 300):\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster the 20,000 word 300 dim dataset\n",
    "with open(\"WordFiles1000/storeXClusters.txt\", 'w') as f:\n",
    "    arrWithClusters = Birch(n_clusters=200).fit_predict(new_x)\n",
    "    for i  in arrWithClusters:\n",
    "        f.write(str(i) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#revealing clusters\n",
    "clusters = [[] for i in range(200)]\n",
    "clustersNum = [[] for i in range(200)]\n",
    "with open(\"WordFiles1000/storeXClusters.txt\", 'r') as f:\n",
    "    for i in range(9999):\n",
    "        s = f.readline()\n",
    "        try:\n",
    "            clusters[int(s)].append(z[i]) #these are indices\n",
    "        except: \n",
    "            pass\n",
    "    \n",
    "        clustersNum[int(s)].append(new_x[i])\n",
    "        \n",
    "\n",
    "with open(\"WordFiles1000/clusterWords.txt\", 'w') as f:\n",
    "    for i in clusters:\n",
    "        if len(i) != 0:\n",
    "            f.write(str(i) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Trial: Threshhold\n",
    "\n",
    "# #Cluster the 20,000 word 300 dim dataset\n",
    "# with open(\"WordFiles2000/clustersThreshhold.txt\", 'w') as f:\n",
    "#     arrWithClusters = Birch(threshold=0.1).fit_predict(new_x)\n",
    "#     for i  in arrWithClusters:\n",
    "#         f.write(str(i) + \"\\n\")\n",
    "\n",
    "# clusters = [[] for i in range(4000)]\n",
    "# with open(\"WordFiles2000/clustersThreshhold.txt\", 'r') as f:\n",
    "#     for i in range(19999):\n",
    "#         s = f.readline()\n",
    "#         try:\n",
    "#             clusters[int(s)].append(z[i]) \n",
    "#         except: \n",
    "#             pass\n",
    "        \n",
    "\n",
    "# with open(\"WordFiles2000/clustersThreshholdWords.txt\", 'w') as f:\n",
    "#     for i in clusters:\n",
    "#         if len(i) != 0:\n",
    "#             f.write(str(i) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ideal clusters - gloveWordsClusters.txt - don't run again\n",
    "# # bash - clusters - NO PCA - stored in clusters.txt\n",
    "\n",
    "# #RUN Bash alg\n",
    "# #bash list of clusters\n",
    "# def cluster_gen(data):\n",
    "#     #swap to snake_case\n",
    "#     touched_points = []\n",
    "#     clusters = []\n",
    "\n",
    "#     for i in range (1000):\n",
    "#         if i in touched_points:\n",
    "#             continue\n",
    "\n",
    "#         curr_cluster = make_cluster(i, data)\n",
    "#         clusters.append(curr_cluster)\n",
    "#         # print(\"check2\")\n",
    "#         #print(curr_cluster)\n",
    "#         with open(\"WordFiles/clusters2.txt\", \"a\") as f:\n",
    "#             for i in range(len(curr_cluster)):\n",
    "#                 f.write(z[curr_cluster[i]] + \" \")\n",
    "#             f.write(\"\\n\")\n",
    "#             for i in range(len(curr_cluster)):\n",
    "#                 f.write(str(curr_cluster[i]) + \" \")\n",
    "#             f.write(\"\\n\")\n",
    "#             f.close()\n",
    "#             #print(z[curr_cluster[i]], end = \" \")\n",
    "#         #print()          \n",
    "\n",
    "#         touched_points.extend(curr_cluster)\n",
    "\n",
    "#     return clusters\n",
    "\n",
    "# def find_most_similar(word, total_words, curr_cluster):\n",
    "\n",
    "#     m = 0\n",
    "#     while m in curr_cluster:\n",
    "#         m = m + 1\n",
    "        \n",
    "#     min_val = cosine(u = total_words[word], v = total_words[m])\n",
    "#     index = m\n",
    "\n",
    "#     for i in range(m+1, 1000):\n",
    "#         skip = False\n",
    "\n",
    "#         if i in curr_cluster:\n",
    "#             skip = True\n",
    "\n",
    "#         curr_val = cosine(u = total_words[word], v = total_words[i])\n",
    "\n",
    "#         if skip:\n",
    "#             curr_val = min_val\n",
    "        \n",
    "#         if curr_val < min_val:\n",
    "#             min_val = curr_val\n",
    "#             index = i\n",
    "\n",
    "#     if min_val > 0.7:\n",
    "#         return word\n",
    "\n",
    "#     return index\n",
    "\n",
    "# def make_cluster(word, data):\n",
    "#     curr_cluster=[]\n",
    "#     visited = []\n",
    "#     queue = []\n",
    "#     queue.append(word)\n",
    "\n",
    "#     while len(curr_cluster) < 7 and queue:\n",
    "#         curr_word = queue.pop(0)\n",
    "\n",
    "#         if curr_word in curr_cluster:\n",
    "#             break\n",
    "\n",
    "#         curr_cluster.append(curr_word)\n",
    "#         visited.append(curr_word)\n",
    "        \n",
    "#         #alg:\n",
    "#         for i in range(4):\n",
    "\n",
    "#             closest_word = find_most_similar(curr_word, data, visited)\n",
    "\n",
    "#             if not (closest_word in curr_cluster):\n",
    "#                 queue.append(closest_word)\n",
    "            \n",
    "#             if closest_word == word:\n",
    "#                 break\n",
    "            \n",
    "#             visited.append(closest_word)\n",
    "\n",
    "#         #alg2:\n",
    "#         visited = []\n",
    "\n",
    "#     # if len(curr_cluster) == 1:\n",
    "#     #     return []\n",
    "    \n",
    "#     return curr_cluster\n",
    "\n",
    "# #cluster_gen(keepX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"WordFiles1000/Clusters300.txt\", 'w') as f:\n",
    "    arrWithClusters = Birch(n_clusters=66).fit_predict(new_x)\n",
    "    for i  in arrWithClusters:\n",
    "        f.write(str(i) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = [[] for i in range(655)]\n",
    "clustersNum = [[] for i in range(655)]\n",
    "with open(\"WordFiles2000/Clusters300.txt\", 'r') as f:\n",
    "    for i in range(19999):\n",
    "        s = f.readline()\n",
    "        try:\n",
    "            clusters[int(s)].append(z[i]) #these are indices\n",
    "        except: \n",
    "            pass\n",
    "    \n",
    "        clustersNum[int(s)].append(new_x[i])\n",
    "        \n",
    "\n",
    "with open(\"WordFiles2000/Clusters100000300Words.txt\", 'w') as f:\n",
    "    for i in clusters:\n",
    "        if len(i) != 0:\n",
    "            f.write(str(i) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcaanal(newDim, arr2):\n",
    "    pca = PCA(n_components = newDim)\n",
    "    arr2 = pca.fit_transform(arr2)\n",
    "    return arr2\n",
    "# def tsneanal(newDim, arr3):\n",
    "#     tsne = TSNE(n_components = newDim, learning_rate = 'auto', init='random', method='exact', perplexity=30).fit_transform(arr3)\n",
    "#     return tsne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pcaanal(300, np.array(new_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dist(X[0], X[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9998):\n",
    "    dist(X[i], X[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9998):\n",
    "    if (dist((X[i]), (X[0])) == 3):\n",
    "        print(\"YAY!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    for j in range(i+1, 100):\n",
    "        dist(X2[i], X2[j])\n",
    "        for k in range(j + 1, 100):\n",
    "            dist(X2[j], X2[k])\n",
    "            for l in range(k + 1, 100):\n",
    "                dist(X2[k], X2[l])\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probably unnecessary...\n",
    "\n",
    "# y = pcaanal(150, keepX)\n",
    "\n",
    "# with open(\"WordFiles/gloveWords150dim.txt\", 'w') as f:\n",
    "#     for i in range(1000):\n",
    "#         f.write(str(y[i]))\n",
    "#         f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_birch2(x, dim):\n",
    "#     brc = Birch(n_clusters=4000)\n",
    "#     x2 = brc.fit_predict(x)\n",
    "#     clusters = [[] for x in range(4000)]\n",
    "#     for i in range(19999):\n",
    "#         clusters[x2[i]].append(z[i])\n",
    "\n",
    "#     with open(\"WordFiles2000/gloveWordsBirchClusters.txt\", 'a') as f:\n",
    "#         f.write('\\n \\n \\n')\n",
    "#         f.write(\" ~DIMENSION~ \" + str(dim) + \"\\n\")\n",
    "#         for i in clusters:\n",
    "#             for j in i:\n",
    "#                 f.write(j)\n",
    "#                 f.write(\" \")\n",
    "#             f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_birch2(keepX)\n",
    "\n",
    "# pcax150 = pcaanal(150, keepX)\n",
    "# run_birch2(pcax150, 150)\n",
    "\n",
    "# pcax33 = pcaanal(33, keepX)\n",
    "# run_birch2(pcax33, 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associations = {}\n",
    "# with open(\"WordFiles2000/clusterWords.txt\", 'r') as f:\n",
    "#     for i in range(4000):\n",
    "#         s = next(f).split(\" \")\n",
    "#         s = s[:len(s) - 1]\n",
    "#         if (len(s) >= 2):\n",
    "#             associations[str(s[0])] = str(s[1])\n",
    "#             #this is just by alphabetical order though. not CLOSEST\n",
    "\n",
    "# print(associations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_birch3(x, dim, s):\n",
    "#     brc = Birch(n_clusters=4000)\n",
    "#     x2 = brc.fit_predict(x)\n",
    "#     clusters = [[] for x in range(4000)]\n",
    "#     index = 0\n",
    "#     for i in range(19999):\n",
    "#         clusters[x2[i]].append(z[i])\n",
    "#         if (z[i] == s):\n",
    "#             index = i\n",
    "#     if(clusters[x2[index]].__contains__(associations[s])):\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import dist\n",
    "\n",
    "def findClosest(pcaxNew0):\n",
    "    minDist = dist(pcaxNew0[0], pcaxNew0[1])\n",
    "    i1 = 0\n",
    "    i2 = 1\n",
    "    for ind1 in range(len(pcaxNew0)):\n",
    "        word1 = pcaxNew0[ind1]\n",
    "        for ind2 in range(ind1+1, len(pcaxNew0)):\n",
    "            word2 = pcaxNew0[ind2]\n",
    "            newDist = dist(word1, word2)\n",
    "            if (minDist > newDist):\n",
    "                minDist = newDist\n",
    "                i1 = ind1\n",
    "                i2 = ind2\n",
    "\n",
    "    return i1, i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#O(n^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findClosest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(findClosest(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(findClosest(new_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def associationConserved(pcaxNew, old):\n",
    "    if findClosest(pcaxNew) == findClosest(old):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "associationConserved(pcaxNew, currClust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaxNew = pcaanal(150, np.array(currClust))\n",
    "pcaxNew = pcaxNew.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import ast\n",
    "# import json\n",
    "# pcaxNew3 = pcaxNew\n",
    "# print(pcaxNew3)\n",
    "# pcaxNew3 = re.sub(r'(?<=\\d)(\\s+)(?=-?\\d)', ',', pcaxNew3)\n",
    "# pcaxNew3 = pcaxNew3.replace(\"\\n\", \",\")\n",
    "# #pcaxNew3 = pcaxNew3.splitlines()\n",
    "# #pcaxNew3 = [[0, -1, -1, 1, 1, 0], [3, 3, 3, 3, 3, 3]]\n",
    "# print(pcaxNew3)\n",
    "# print(type(pcaxNew3))\n",
    "# pcaxNew3 = ast.literal_eval(str(pcaxNew3))\n",
    "# print(pcaxNew3)\n",
    "# print(type(pcaxNew3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaxNew = pcaanal(300, new_x)\n",
    "pcaxNew = pcaxNew.tolist()\n",
    "print(\"done\")\n",
    "associationConserved(pcaxNew, currClust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(n, s):\n",
    "    low = 1\n",
    "    high = n\n",
    "    mid = 0\n",
    "\n",
    "    while low <= high:\n",
    "        mid = (high+low)//2\n",
    "        #print(mid)\n",
    "        pcaxNew = pcaanal(mid, currClust)\n",
    "        pcaxNew = pcaxNew.tolist()\n",
    "        #print(\"done\")\n",
    "        if associationConserved(pcaxNew, s):\n",
    "        #run_birch3(pcaxNew, mid, s)\n",
    "            high = mid - 1\n",
    "        else:\n",
    "            low = mid + 1\n",
    "    \n",
    "    return mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"WordFiles2000/answer100000.txt\", 'w') as f:\n",
    "    for currClust in clustersNum:\n",
    "        #print(len(currClust))\n",
    "        if     (len(currClust) >= 300):\n",
    "            abcd = binary_search(300, currClust)\n",
    "            print(str(abcd))\n",
    "            f.write(str(abcd))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clustersNum[1])\n",
    "print(clusters[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in associations:\n",
    "    ans = binary_search(300, key)\n",
    "    print(ans)\n",
    "    with open (\"WordFiles2000/Dimensions.txt\", 'a') as f:\n",
    "        f.write(str(ans))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "#Wait so the ISSUE is that the code isn't fast enough right. so i'm only looking at one word epr cluster BUT ALL I HAVE TO DO DIMENSIONALLY IS REDUCE THE SIZE OF THE CLUSTER\n",
    "#Or the size of the cluster's words (all words int he cluster) and confirm that the two words more closely associated are still most closely associated\n",
    "#So I want to fracture my matrix of words into clusters. right. like given a cluster, I take all datapoints in that cluster and reduce birch wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open (\"WordFiles/Dimensions2.txt\", 'r') as f:\n",
    "    dims = f.read().split(\"\\n\")\n",
    "\n",
    "dims = dims[:len(dims) - 1]\n",
    "print(dims)\n",
    "\n",
    "for i in range(len(dims)):\n",
    "    dims[i] = int(dims[i])\n",
    "\n",
    "if (dims[i] < 3):\n",
    "    print(3)\n",
    "else:\n",
    "    print(dims[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations2 = {}\n",
    "with open(\"WordFiles/gloveWordsBirch300.txt\", 'r') as f:\n",
    "    for i in range(142):\n",
    "        s = next(f).split(\" \")\n",
    "        s = s[:len(s) - 1]\n",
    "        if (len(s) >= 3):\n",
    "            associations2[str(s[0])] = str(s[2])\n",
    "       \n",
    "\n",
    "print(associations2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "for key in associations2:\n",
    "    ans = binary_search(300, key)\n",
    "    print(ans)\n",
    "    with open (\"WordFiles/Dimensions2.txt\", 'a') as f:\n",
    "        if (ans < dims[ind]):\n",
    "            f.write(str(ans))\n",
    "        else:\n",
    "            f.write(str(dims[ind]))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    ind = ind + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_search(300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b035a97ad67dff3ddcf42b4508c859b33be75c134f59f08fe6dd0d28f1650ecb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
